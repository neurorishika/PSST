
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Day 2: Let the Tensors Flow! &#8212; PSST: Parallelised Scalable Simulations in TensorFlow</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/PSST-favicon-32x32.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Day 3: Cells in Silicon" href="../Day%203%20Cells%20in%20Silicon/Day%203.html" />
    <link rel="prev" title="Day 1: Of Numerical Integration and Python" href="../Day%201%20Of%20Numerical%20Integration%20and%20Python/Day%201.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/PSST.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">PSST: Parallelised Scalable Simulations in TensorFlow</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../README.html">
   PSST … It’s well Documented!
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README.html">
   Day 0 : Introduction to the Tutorials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Day%201%20Of%20Numerical%20Integration%20and%20Python/Day%201.html">
   Day 1: Of Numerical Integration and Python
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Day 2: Let the Tensors Flow!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Day%203%20Cells%20in%20Silicon/Day%203.html">
   Day 3: Cells in Silicon
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Day%204%20Neurons%20and%20Networks/Day%204.html">
   Day 4: Neurons and Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Day%205%20Optimal%20Mind%20Control/Day%205.html">
   Day 5: Optimal Mind Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Example%20Implementation%20Locust%20AL/Example.html">
   Day 6: (Example Implementation) Into the Mind of a Locust
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Optional%20Material/Distributed%20TensorFlow/Distributed%20TensorFlow.html">
   Day 7: (Optional) Distributed Computing with TensorFlow
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/Tutorial/Day 2 Let the Tensors Flow/Day 2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/neurorishika/PSST"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/neurorishika/PSST/issues/new?title=Issue%20on%20page%20%2FTutorial/Day 2 Let the Tensors Flow/Day 2.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#an-introduction-to-tensorflow">
   An Introduction to TensorFlow
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-gpu-vs-cpu">
     Why GPU vs CPU?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-tensorflow-works">
     How TensorFlow works?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-a-computational-graph-in-tensorflow">
     Implementing a computational graph in TensorFlow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#efficient-recursion-with-tensorflow">
     Efficient recursion with TensorFlow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#euler-integration-function-in-tensorflow">
     Euler Integration Function in TensorFlow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#running-the-tensorflow-euler-integrator">
     Running the TensorFlow Euler Integrator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rk4-integration-function-in-tensorflow">
     RK4 Integration Function in TensorFlow
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-information-about-computational-hardware">
   Additional Information about Computational Hardware:
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#about-tpu-tensor-processing-units">
     About TPU (Tensor Processing Units):
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Day 2: Let the Tensors Flow!</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#an-introduction-to-tensorflow">
   An Introduction to TensorFlow
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-gpu-vs-cpu">
     Why GPU vs CPU?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-tensorflow-works">
     How TensorFlow works?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-a-computational-graph-in-tensorflow">
     Implementing a computational graph in TensorFlow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#efficient-recursion-with-tensorflow">
     Efficient recursion with TensorFlow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#euler-integration-function-in-tensorflow">
     Euler Integration Function in TensorFlow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#running-the-tensorflow-euler-integrator">
     Running the TensorFlow Euler Integrator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rk4-integration-function-in-tensorflow">
     RK4 Integration Function in TensorFlow
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-information-about-computational-hardware">
   Additional Information about Computational Hardware:
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#about-tpu-tensor-processing-units">
     About TPU (Tensor Processing Units):
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><a href="https://colab.research.google.com/github/neurorishika/PSST/blob/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/Day%202.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/Day%202.ipynb" target="_parent"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg" /></a></p>
<div class="section" id="day-2-let-the-tensors-flow">
<h1>Day 2: Let the Tensors Flow!<a class="headerlink" href="#day-2-let-the-tensors-flow" title="Permalink to this headline">¶</a></h1>
<p>Welcome to Day 2! Today, we start with our discussion with an introduction to TensorFlow followed by implementation of Numerical Integration techniques in TensorFlow.</p>
<div class="section" id="an-introduction-to-tensorflow">
<h2>An Introduction to TensorFlow<a class="headerlink" href="#an-introduction-to-tensorflow" title="Permalink to this headline">¶</a></h2>
<p>TensorFlow is an open-source library that was developed by researchers and engineers in the Google Brain team. TensorFlow has a number of functions that make it particularly suitable for machine learning applications. However, it is primarily an interface for numerical computation. TensorFlow was designed with scalablity in mind. Code written using TensorFlow functions can work seamlessly on single cores, multi-core shared memory processors, high-performance computer clusters, and GPUs.</p>
<div class="section" id="why-gpu-vs-cpu">
<h3>Why GPU vs CPU?<a class="headerlink" href="#why-gpu-vs-cpu" title="Permalink to this headline">¶</a></h3>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/GPUvsCPU.svg"><img alt="https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/GPUvsCPU.svg" src="https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/GPUvsCPU.svg" width="600" /></a>
<p>The answer lies in the architecture:</p>
<p>ALUs (Arithmetic/Logical Units, i.e. CPU Cores/CUDA Cores) are the main circuits that perform all the computations. CPUs (left panel) have fewer ALUs than GPUs (right panel). But, at the same time, the CPU cores can run at a higher clock speed and can do more complex calculations. On the other hand, GPU cores are specialized for linear algebra especially vector operations. Furthermore, a single CPU processor has common memory and control units that must be refreshed every time a new thread of operations is executed. In contrast, GPUs have a hierarchical structure with multiple memory and control units, making them faster at executing multiple threads of operations. This makes GPUs better for simple but highly repetitive tasks executed in parallel, while the CPU is suitable for large complex computations executed serially.</p>
<p>In Summary,</p>
<p><strong>CPU = Faster per Core Processing, Slow but Large Memory Buffer, Few Cores</strong><br />
<strong>GPU = Slower Processing, Faster but Smaller Memory Buffer, Many Cores</strong></p>
<p>Thus GPUs are optimized for large number of simple calculations done in parallel. The extent of this parallelization makes it suitable for vector/tensor manipulation.</p>
</div>
<div class="section" id="how-tensorflow-works">
<h3>How TensorFlow works?<a class="headerlink" href="#how-tensorflow-works" title="Permalink to this headline">¶</a></h3>
<p>All computations in TensorFlow are specified as directed graphs (nodes connected by arrows) known as data flow graphs. Nodes are operations such as addition, multiplication etc. The incoming edges for each node are tensors (scalars, vectors, matrices and higher dimensional arrays), the actual values that are operated upon. The output is also a tensor that results from the computation. For example, consider the following computation where two vectors <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> serve as inputs to the node, a matrix multiplication operation, that produces a matrix <span class="math notranslate nohighlight">\(c\)</span> as output.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/matmul.svg"><img alt="https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/matmul.svg" src="https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/matmul.svg" width="300" /></a>
<p>Here, “matmul” is a node which represents the matrix multiplication operation. a and b are input matrices (2-D tensors) and c is the resultant matrix.</p>
</div>
<div class="section" id="implementing-a-computational-graph-in-tensorflow">
<h3>Implementing a computational graph in TensorFlow<a class="headerlink" href="#implementing-a-computational-graph-in-tensorflow" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, import Numpy and Matplotlib</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Import TensorFlow</span>
<span class="c1"># When using TensorFlow 2.x on both CPU only or single GPU setup, </span>
<span class="c1"># we suggest using TensorFlow v1 Compatibility instead as follows:</span>
<span class="kn">import</span> <span class="nn">tensorflow.compat.v1</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span>

<span class="c1"># This is because of better multithreading optimization and disabled </span>
<span class="c1"># eager execution in the earlier versions of TensorFlow which gives</span>
<span class="c1"># better performance. </span>

<span class="c1"># Defining Input Matrices</span>
<span class="n">a_</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">],[</span><span class="mf">3.</span><span class="p">]]</span> <span class="c1"># a 3x1 column matrix </span>
<span class="n">b_</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">]]</span> <span class="c1"># a 1x3 row matrix </span>

<span class="c1"># Creating nodes in the computation graph </span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">a_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="c1"># 3x1 tensor</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="c1"># 1x3 tensor</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> 

<span class="c1"># In TensorFlow v1.x compatible code, </span>
<span class="c1"># To run the graph, we need to create a session.</span>
<span class="c1"># Creating the session initializes the computational device.</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="c1"># start a session</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="c1"># compute the value of c</span>
<span class="n">sess</span><span class="o">.</span><span class="n">close</span><span class="p">()</span> <span class="c1"># end the session</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1. 2. 3.]
 [2. 4. 6.]
 [3. 6. 9.]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="efficient-recursion-with-tensorflow">
<h3>Efficient recursion with TensorFlow<a class="headerlink" href="#efficient-recursion-with-tensorflow" title="Permalink to this headline">¶</a></h3>
<p>To iterate over a list in Python, we used a “for” loop. However, when we implement the same in TensorFlow, putting operations inside a loop replicates the defined computation and chains them together. This results in a long repetitive computation graph with the same operations chained one after the other, resulting in large memory usage and slow computation. TensorFlow provides an alternative with the tf.scan() method.</p>
<p>Say, one wants to recursively apply a function on an initial value but the function takes in additional input at every recursive call, for example, to find the cumulative sum over a list. Every step adds a new element from the list to the last addition. The TensorFlow function tf.scan allows us to easily implement such an iterator.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define the recursive function that takes in two values the</span>
<span class="c1"># accumulated value and the additional input from a list.</span>
<span class="k">def</span> <span class="nf">recursive_addition</span><span class="p">(</span><span class="n">accumulator</span><span class="p">,</span><span class="n">new_element</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function takes in two values the accumulated value and the additional input which is added onto the accumulated value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">accumulator</span><span class="o">+</span><span class="n">new_element</span>

<span class="c1"># define the list over which we iterate</span>
<span class="n">elems</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>

<span class="c1"># tf.scan takes in three inputs: the recursive function, the </span>
<span class="c1"># list to iterate over and the initial value. If an initial </span>
<span class="c1"># value is not provided, its taken as the first element of elems.</span>

<span class="c1"># accumulate with no initializer</span>
<span class="n">cum_sum_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">recursive_addition</span><span class="p">,</span> <span class="n">elems</span><span class="p">)</span> 
<span class="c1"># accumulate with initializer as the number 5</span>
<span class="n">cum_sum_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">recursive_addition</span><span class="p">,</span> <span class="n">elems</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>

<span class="c1"># To automatically close the session after computation in TensorFlow 1.13, Use:</span>
<span class="c1"># with tf.Session() as sess:</span>
<span class="c1">#    output = sess.run(c) </span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">output_a</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cum_sum_a</span><span class="p">)</span>
    <span class="n">output_b</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cum_sum_b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output_a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 1  3  6 10 15 21]
[ 6  8 11 15 20 26]
</pre></div>
</div>
</div>
</div>
<p>As an <strong>Exercise</strong> use tf.scan to compute the fibonacci sequence.</p>
</div>
<div class="section" id="euler-integration-function-in-tensorflow">
<h3>Euler Integration Function in TensorFlow<a class="headerlink" href="#euler-integration-function-in-tensorflow" title="Permalink to this headline">¶</a></h3>
<p>We now implement Euler’s method using tf.scan to iterate over the time array. Note that the function scan_func that defines each step of Euler’s method, is now an input to tf.scan.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tf_check_type</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y0</span><span class="p">):</span> <span class="c1"># Ensure Input is Correct</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function checks the type of the input to ensure that it is a floating point number.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">y0</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">):</span> 
        <span class="c1"># The datatype of any tensor t is accessed by t.dtype</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Error: y0 and t must be floating point numbers&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">_Tf_Integrator</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class for integrating a function using the Euler method in TensorFlow.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">integrate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function integrates a function func using the Euler method in TensorFlow.</span>
<span class="sd">        </span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        func: function</span>
<span class="sd">            The function to be integrated.</span>
<span class="sd">        y0: float</span>
<span class="sd">            The initial condition.</span>
<span class="sd">        t: numpy array</span>
<span class="sd">            The time array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">time_delta_grid</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">t</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># define the time step at each point</span>
        
        <span class="k">def</span> <span class="nf">scan_func</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t_dt</span><span class="p">):</span>  <span class="c1"># define the scan function that performs the integration step</span>
            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            This function performs the integration step.</span>

<span class="sd">            Parameters:</span>
<span class="sd">            -----------</span>
<span class="sd">            y: float</span>
<span class="sd">                The value of y at which the function is being evaluated.</span>
<span class="sd">            t_dt: (float, float)</span>
<span class="sd">                The time point and time step at which the function is being evaluated.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">t</span><span class="p">,</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">t_dt</span> <span class="c1"># unpack the time point and time step</span>
            <span class="n">dy</span> <span class="o">=</span> <span class="n">dt</span><span class="o">*</span><span class="n">func</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">t</span><span class="p">)</span> <span class="c1"># compute the change in function at the current time point</span>
            <span class="k">return</span> <span class="n">y</span> <span class="o">+</span> <span class="n">dy</span> <span class="c1"># return the new value of y</span>
        
        <span class="c1"># iterating over (a,b) where a and b are lists of same size</span>
        <span class="c1"># results in the ith accumulative step in tf.scan receiving</span>
        <span class="c1"># the ith elements of a and b zipped together</span>
        
        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">scan_func</span><span class="p">,</span> <span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">time_delta_grid</span><span class="p">),</span><span class="n">y0</span><span class="p">)</span> <span class="c1"># perform the integration using tf.scan</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">y0</span><span class="p">],</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># add the initial condition at the beginning and return the values</span>

<span class="k">def</span> <span class="nf">tf_odeint_euler</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function integrates the function func using the Euler method implemented in the _Tf_Integrator class.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    func: function</span>
<span class="sd">        The function to be integrated.</span>
<span class="sd">    y0: float</span>
<span class="sd">        The initial condition.</span>
<span class="sd">    t: numpy array</span>
<span class="sd">        The time array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Ensure Input is in the form of TensorFlow Tensors</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;t&#39;</span><span class="p">)</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;y0&#39;</span><span class="p">)</span>
    <span class="n">tf_check_type</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span><span class="n">t</span><span class="p">)</span> <span class="c1"># Ensure Input is of the correct type</span>
    <span class="k">return</span> <span class="n">_Tf_Integrator</span><span class="p">()</span><span class="o">.</span><span class="n">integrate</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="n">y0</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="running-the-tensorflow-euler-integrator">
<h3>Running the TensorFlow Euler Integrator<a class="headerlink" href="#running-the-tensorflow-euler-integrator" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a function using Tensorflow math operations. This creates a computational graph.</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">t</span><span class="p">):</span>
    <span class="c1"># extracting a single value eg. X[0] returns a single value but</span>
    <span class="c1"># we require a tensor, so we extract a range with one element.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span> 
    <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">,</span><span class="n">y</span><span class="o">-</span><span class="n">x</span><span class="p">],</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">y0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">epsilon</span><span class="p">)</span>

<span class="c1"># Define the final value (output of scan) that we wish to compute as a variable</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">tf_odeint_euler</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">y0</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>

<span class="c1"># Start a TF session and evaluate state</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<span class="c1"># Plot the solution</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[::</span><span class="mi">5</span><span class="p">],</span><span class="n">state</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">,::</span><span class="mi">5</span><span class="p">],</span><span class="s2">&quot;.&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Eulers Solution for x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[::</span><span class="mi">5</span><span class="p">],</span><span class="n">state</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">,::</span><span class="mi">5</span><span class="p">],</span><span class="s2">&quot;.&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Eulers Solution for y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;t&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Day 2_10_0.png" src="../../_images/Day 2_10_0.png" />
</div>
</div>
</div>
<div class="section" id="rk4-integration-function-in-tensorflow">
<h3>RK4 Integration Function in TensorFlow<a class="headerlink" href="#rk4-integration-function-in-tensorflow" title="Permalink to this headline">¶</a></h3>
<p>Now, we implement the RK4 integrator in TensorFlow. Note that here we replace the single step iterator used for the Euler’s with a four step RK4 iterator. In addition, to make the code more modular, we define a function _step_func() that is called by scan_func and calculates the next step of the RK4 integrator. The rest of the program remains the same as the Euler’s method implemented above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tf_check_type</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y0</span><span class="p">):</span> <span class="c1"># Ensure Input is Correct</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function checks the type of the input to ensure that it is a floating point number.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">y0</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Error: y0 and t must be floating point numbers&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">_Tf_Integrator</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class implements the Runge-Kutta 4th order method in TensorFlow.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">integrate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function integrates a function func using the Runge-Kutta 4th order method in TensorFlow.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        func: function</span>
<span class="sd">            The function to be integrated.</span>
<span class="sd">        y0: float</span>
<span class="sd">            The initial condition.</span>
<span class="sd">        t: numpy array</span>
<span class="sd">            The time array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">time_delta_grid</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">t</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># define the time step at each point</span>
        
        <span class="k">def</span> <span class="nf">scan_func</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t_dt</span><span class="p">):</span> <span class="c1"># define the scan function that performs the integration step</span>
            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            This function performs the integration step.</span>
<span class="sd">            </span>
<span class="sd">            Parameters:</span>
<span class="sd">            -----------</span>
<span class="sd">            y: float</span>
<span class="sd">                The value of y at which the function is being evaluated.</span>
<span class="sd">            t_dt: (float, float)</span>
<span class="sd">                The time point and time step at which the function is being evaluated.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">t</span><span class="p">,</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">t_dt</span> <span class="c1"># unpack the time point and time step</span>
            <span class="n">dy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_func</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="n">dt</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1"># Make code more modular.</span>
            <span class="k">return</span> <span class="n">y</span> <span class="o">+</span> <span class="n">dy</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">scan_func</span><span class="p">,</span> <span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">time_delta_grid</span><span class="p">),</span><span class="n">y0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">y0</span><span class="p">],</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_step_func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function determines the value of the integration step.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        func: function</span>
<span class="sd">            The function to be integrated.</span>
<span class="sd">        t: float</span>
<span class="sd">            The time point at which the function is being evaluated.</span>
<span class="sd">        dt: float</span>
<span class="sd">            The time step at which the function is being integrated.</span>
<span class="sd">        y: float</span>
<span class="sd">            The value of y at which the function is being evaluated.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">k1</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">half_step</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">dt_cast</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="c1"># Failsafe</span>

        <span class="n">k2</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">dt_cast</span> <span class="o">*</span> <span class="n">k1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">half_step</span><span class="p">)</span>
        <span class="n">k3</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">dt_cast</span> <span class="o">*</span> <span class="n">k2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">half_step</span><span class="p">)</span>
        <span class="n">k4</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">dt_cast</span> <span class="o">*</span> <span class="n">k3</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="n">dt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">([</span><span class="n">k1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k3</span><span class="p">,</span> <span class="n">k4</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="n">dt_cast</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span> <span class="c1"># add all update terms</span>
    
<span class="k">def</span> <span class="nf">tf_odeint_rk4</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function integrates the function func using the Runge-Kutta 4th order method implemented in the _Tf_Integrator class.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    func: function</span>
<span class="sd">        The function to be integrated.</span>
<span class="sd">    y0: float</span>
<span class="sd">        The initial condition.</span>
<span class="sd">    t: numpy array</span>
<span class="sd">        The time array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Ensure Input is in the form of TensorFlow Tensors</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;t&#39;</span><span class="p">)</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;y0&#39;</span><span class="p">)</span>
    <span class="n">tf_check_type</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span><span class="n">t</span><span class="p">)</span> <span class="c1"># Ensure Input is of the correct type</span>
    <span class="k">return</span> <span class="n">_Tf_Integrator</span><span class="p">()</span><span class="o">.</span><span class="n">integrate</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="n">y0</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a function using Tensorflow math operations. This creates a computational graph.</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">t</span><span class="p">):</span>
    <span class="c1"># extracting a single value eg. X[0] returns a single value but</span>
    <span class="c1"># we require a tensor, so we extract a range with one element.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span> 
    <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">,</span><span class="n">y</span><span class="o">-</span><span class="n">x</span><span class="p">],</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">y0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">epsilon</span><span class="p">)</span>

<span class="c1"># Define the final value (output of scan) that we wish to compute as a variable</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">tf_odeint_rk4</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">y0</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>

<span class="c1"># Start a TF session and evaluate state</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<span class="c1"># Plot the solution</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[::</span><span class="mi">5</span><span class="p">],</span><span class="n">state</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">,::</span><span class="mi">5</span><span class="p">],</span><span class="s2">&quot;.&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;RK4 Solution for x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[::</span><span class="mi">5</span><span class="p">],</span><span class="n">state</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">,::</span><span class="mi">5</span><span class="p">],</span><span class="s2">&quot;.&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;RK4 Solution for y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;t&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Day 2_13_0.png" src="../../_images/Day 2_13_0.png" />
</div>
</div>
<p><strong>Exercise</strong> Simulate the non-linear Lorentz Attractor using Euler Method and RK4 on TensorFlow which is given by the equations:</p>
<div class="math notranslate nohighlight">
\[\frac{dx}{dt}=\sigma(y-x)\]</div>
<div class="math notranslate nohighlight">
\[\frac{dy}{dt}=x(\rho-z)-y\]</div>
<div class="math notranslate nohighlight">
\[\frac{dz}{dt}=xy-\beta z\]</div>
<p>Use the values <span class="math notranslate nohighlight">\(\sigma =10\)</span>, <span class="math notranslate nohighlight">\(\beta =\frac{8}{3}\)</span>, <span class="math notranslate nohighlight">\(\rho =28\)</span>. You can try simulating this system at two nearby starting conditions and comment on the difference.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/lorenz.svg"><img alt="https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/lorenz.svg" src="https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%202%20Let%20the%20Tensors%20Flow/lorenz.svg" width="400" /></a>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#####################################</span>
<span class="c1"># Problem Hint for the Euler Method #</span>
<span class="c1">#####################################</span>

<span class="c1"># # Note: Uncommment and complete the code below to practice the Euler method to simulate a lorenz attractor.</span>

<span class="c1"># epsilon  = ... # define the resolution of the simulation</span>
<span class="c1"># t = ... # define the time array</span>

<span class="c1"># sigma = ... # define the sigma parameter</span>
<span class="c1"># beta = ... # define the beta parameter</span>
<span class="c1"># rho = ... # define the rho parameter</span>

<span class="c1"># def F(X,t): # Create a new function F(X,t) where X is a vector of the form [theta,omega] and t is the time. </span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     This function defines the differential equation for the lorenz attractor.</span>

<span class="c1">#     Parameters:</span>
<span class="c1">#     -----------</span>
<span class="c1">#     X: numpy array</span>
<span class="c1">#         The state vector of the lorenz attractor given by [x,y,z].</span>
<span class="c1">#     t: float</span>
<span class="c1">#         The time.</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     x = ... # extract the x variable from the state vector</span>
<span class="c1">#     y = ... # extract the y variable from the state vector</span>
<span class="c1">#     z = ... # extract the z variable from the state vector</span>
<span class="c1">#     dx = ... # define the derivative of x</span>
<span class="c1">#     dy = ... # define the derivative of y</span>
<span class="c1">#     dz = ... # define the derivative of z</span>
<span class="c1">#     return np.array([dx,dy,dz]) # return the derivative of the state vector</span>

<span class="c1"># solution = tf_odeint_euler(...) # Use the Euler method to solve the pendulum problem</span>

<span class="c1"># # Plot the solution in 3D</span>
<span class="c1"># from mpl_toolkits import mplot3d</span>

<span class="c1"># ax = plt.axes(projection=&#39;3d&#39;)</span>
<span class="c1"># ax.plot3D(solution[0,:],solution[1,:],solution[2,:],label=&quot;Euler Solution for x,y,z in 3D&quot;)</span>
<span class="c1"># plt.xlabel(&quot;t&quot;)</span>
<span class="c1"># plt.ylabel(&quot;theta/omega&quot;)</span>
<span class="c1"># plt.legend()</span>
<span class="c1"># plt.show()</span>

<span class="c1"># # Now try to implement the Runge-Kutta of order 4 method to solve the same problem.</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="additional-information-about-computational-hardware">
<h2>Additional Information about Computational Hardware:<a class="headerlink" href="#additional-information-about-computational-hardware" title="Permalink to this headline">¶</a></h2>
<div class="section" id="about-tpu-tensor-processing-units">
<h3>About TPU (Tensor Processing Units):<a class="headerlink" href="#about-tpu-tensor-processing-units" title="Permalink to this headline">¶</a></h3>
<p>In recent years, there have been a lot of advancements in the hardware available for large-scale computing. In 2016, Google announced a new hardware architecture known as TPUs (Tensor Processing Units) that were specifically designed for use with the TensorFlow library. TPUs utilize a specialized Matrix Multiplication Unit that maximizes parallel computation and are capable of much faster matrix multiplication than CPUs or GPUs. As a result, TPUs can further speed up the the integration of ODEs and other numerical simulations. While, the code we presented in this tutorial can be run on a system with available TPUs, the allocation of tasks to different TPU cores is not currently automated and must be done manually. And so, using TPUs efficiently may require a few additional steps which are outside the scope of this tutorial. However, as TPUs are becoming more popular, it is likely that future versions of the TensorFlow library will be able to automatically allocate tasks to different TPU cores.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Tutorial\Day 2 Let the Tensors Flow"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../Day%201%20Of%20Numerical%20Integration%20and%20Python/Day%201.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Day 1: Of Numerical Integration and Python</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../Day%203%20Cells%20in%20Silicon/Day%203.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Day 3: Cells in Silicon</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Rishika Mohanta and Collins Assisi<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>