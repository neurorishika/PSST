@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@Misc{tensorflow-api-docs,
title={ {TensorFlow} API docs},
url={www.tensorflow.org/api_docs/python},
}

@inbook{tfcookbook,
author = {McClure, Nick},
title = {TensorFlow Machine Learning Cookbook},
booktitle = {TensorFlow Machine Learning Cookbook},
year = {2018},
publisher = {Packt Publishing},
chapter = {11},
}

@article{Ekmekci2016,
abstract = {Computing has revolutionized the biological sciences over the past several decades, such that virtually all contemporary research in the biosciences utilizes computer programs. The computational advances have come on many fronts, spurred by fundamental developments in hardware, software, and algorithms. These advances have influenced, and even engendered, a phenomenal array of bioscience fields, including molecular evolution and bioinformatics; genome-, proteome-, transcriptome- and metabolome-wide experimental studies; structural genomics; and atomistic simulations of cellular-scale molecular assemblies as large as ribosomes and intact viruses. In short, much of post-genomic biology is increasingly becoming a form of computational biology. The ability to design and write computer programs is among the most indispensable skills that a modern researcher can cultivate. Python has become a popular programming language in the biosciences, largely because (i) its straightforward semantics and clean syntax make it a readily accessible first language; (ii) it is expressive and well-suited to object-oriented programming, as well as other modern paradigms; and (iii) the many available libraries and third-party toolkits extend the functionality of the core language into virtually every biological domain (sequence and structure analyses, phylogenomics, workflow management systems, etc.). This primer offers a basic introduction to coding, via Python, and it includes concrete examples and exercises to illustrate the language's usage and capabilities; the main text culminates with a final project in structural bioinformatics. A suite of Supplemental Chapters is also provided. Starting with basic concepts, such as that of a 'variable', the Chapters methodically advance the reader to the point of writing a graphical user interface to compute the Hamming distance between two DNA sequences.},
author = {Ekmekci, Berk and McAnany, Charles E. and Mura, Cameron},
doi = {10.1371/journal.pcbi.1004867},
file = {:Users/collins/Documents/watchFolderMendeley/journal.pcbi.1004867.PDF:PDF},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {6},
pages = {1--43},
title = {{An Introduction to Programming for Bioscientists: A Python-Based Primer}},
volume = {12},
year = {2016}
}

@article{Bassi2007,
abstract = {Accessed 4 October 2007. Perez F, Granger BE (2007) : A for . CiSE 9: 21-29. Find},
author = {Bassi, Sebastian},
doi = {10.1371/journal.pcbi.0030199},
issn = {1553-734X},
journal = {PLoS Computational Biology},
title = {{A Primer on Python for Life Science Researchers}},
year = {2007}
}

@book{primer,
author = {Langtangen Hans, Petter},
title = {A Primer on Scientific Programming with Python},
year = {2009},
volume = {6},
series = {Texts in Computational Science and Engineering},
publisher = {Springer},
}

@article{Burak2009,
abstract = {Grid cells in the rat entorhinal cortex display strikingly regular firing responses to the animal's position in 2-D space and have been hypothesized to form the neural substrate for dead-reckoning. However, errors accumulate rapidly when velocity inputs are integrated in existing models of grid cell activity. To produce grid-cell-like responses, these models would require frequent resets triggered by external sensory cues. Such inadequacies, shared by various models, cast doubt on the dead-reckoning potential of the grid cell system. Here we focus on the question of accurate path integration, specifically in continuous attractor models of grid cell activity. We show, in contrast to previous models, that continuous attractor models can generate regular triangular grid responses, based on inputs that encode only the rat's velocity and heading direction. We consider the role of the network boundary in the integration performance of the network and show that both periodic and aperiodic networks are capable of accurate path integration, despite important differences in their attractor manifolds. We quantify the rate at which errors in the velocity integration accumulate as a function of network size and intrinsic noise within the network. With a plausible range of parameters and the inclusion of spike variability, our model networks can accurately integrate velocity inputs over a maximum of approximately 10-100 meters and approximately 1-10 minutes. These findings form a proof-of-concept that continuous attractor dynamics may underlie velocity integration in the dorsolateral medial entorhinal cortex. The simulations also generate pertinent upper bounds on the accuracy of integration that may be achieved by continuous attractor dynamics in the grid cell network. We suggest experiments to test the continuous attractor model and differentiate it from models in which single cells establish their responses independently of each other.},
archivePrefix = {arXiv},
arxivId = {0811.1826},
author = {Burak, Yoram and Fiete, Ila R.},
doi = {10.1371/journal.pcbi.1000291},
eprint = {0811.1826},
file = {:Users/collins/Documents/watchFolderMendeley/journal.pcbi.1000291.PDF:PDF},
isbn = {1553-7358 (Electronic)$\backslash$n1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {2},
pmid = {19229307},
title = {{Accurate path integration in continuous attractor network models of grid cells}},
volume = {5},
year = {2009}
}


@article{Bazhenov2001,
abstract = {Transient pairwise synchronization of locust antennal lobe (AL) projection neurons (PNs) occurs during odor responses. In a Hodgkin-Huxley-type model of the AL, interactions between excitatory PNs and inhibitory local neurons (LNs) created coherent network oscillations during odor stimulation. GABAergic interconnections between LNs led to competition among them such that different groups of LNs oscillated with periodic Ca(2+) spikes during different 50-250 ms temporal epochs, similar to those recorded in vivo. During these epochs, LN-evoked IPSPs caused phase-locked, population oscillations in sets of postsynaptic PNs. The model shows how alternations of the inhibitory drive can temporally encode sensory information in networks of neurons without precisely tuned intrinsic oscillatory properties.},
address = {Howard Hughes Medical Institute, The Salk Institute, Computational Neurobiology Laboratory, La Jolla, CA 92037, USA. bazhenov@salk.edu},
author = {Bazhenov, M and Stopfer, M and Rabinovich, M and Huerta, R and Abarbanel, H D and Sejnowski, T J and Laurent, G},
file = {:Users/collins/Library/Application Support/Mendeley Desktop/Downloaded/Bazhenov et al. - 2001 - Model of transient oscillatory synchronization in the locust antennal lobe.pdf:pdf},
journal = {Neuron},
number = {2},
pages = {553--567},
publisher = {Elsevier},
title = {{Model of transient oscillatory synchronization in the locust antennal lobe.}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=11395014{\&}retmode=ref{\&}cmd=prlinks papers2://publication/uuid/CE874FDC-F9D0-48F4-9757-3F883ABFC6B4},
volume = {30},
year = {2001}
}

@article{Neru2019,
abstract = {Reliable sequential activity of neurons in the entorhinal cortex is necessary to encode spatially guided behavior and memory. In a realistic computational model of a medial entorhinal cortex (MEC) microcircuit, with stellate cells coupled via a network of inhibitory interneurons, we show how intrinsic and network mechanisms interact with theta oscillations to generate reliable outputs. Sensory inputs activate interneurons near their most excitable phase during each theta cycle. As the inputs change, different groups of interneurons are recruited and postsynaptic stellate cells are released from inhibition causing a sequence of rebound spikes. Since the rebound time scale of stellate cells matches theta oscillations, its spikes get relegated to the least excitable phase of theta ensuring that the network encodes only the external drive and ignores recurrent excitation by rebound spikes. In the absence of theta, rebound spikes compete with external inputs and disrupt the sequence that follows. Our simulations concur with experimental data that show, subduing theta oscillations disrupts the spatial periodicity of grid cell receptive fields. Further, the same mechanism where theta modulates the gain of incoming inputs may be used to select between competing sources of input and create transient functionally connected networks.},
author = {Neru, Arun and Assisi, Collins},
doi = {10.1101/545822},
file = {:Users/collins/Documents/watchFolderMendeley/545822.full.pdf:pdf},
journal = {bioRxiv},
pages = {545822},
title = {{Theta oscillations gate the transmission of reliable sequences in the medial entorhinal cortex}},
url = {https://www.biorxiv.org/content/10.1101/545822v1.abstract},
year = {2019}
}

@article{Bazhenov2005,
abstract = {Recordings in the locust antennal lobe (AL) reveal activity-dependent, stimulus-specific changes in projection neuron (PN) and local neuron response patterns over repeated odor trials. During the first few trials, PN response intensity decreases, while spike time precision increases, and coherent oscillations, absent at first, quickly emerge. We examined this "fast odor learning" with a realistic computational model of the AL. Activity-dependent facilitation of AL inhibitory synapses was sufficient to simulate physiological recordings of fast learning. In addition, in experiments with noisy inputs, a network including synaptic facilitation of both inhibition and excitation responded with reliable spatiotemporal patterns from trial to trial despite the noise. A network lacking fast plasticity, however, responded with patterns that varied across trials, reflecting the input variability. Thus, our study suggests that fast olfactory learning results from stimulus-specific, activity-dependent synaptic facilitation and may improve the signal-to-noise ratio for repeatedly encountered odor stimuli.},
address = {The Salk Institute for Biological Studies, Computational Neurobiology Laboratory, La Jolla, California 92037, USA. bazhenov@salk.edu},
author = {Bazhenov, Maxim and Stopfer, Mark and Sejnowski, Terrence J and Laurent, Gilles},
file = {:Users/collins/Documents/watchFolderMendeley/1-s2.0-S0896627305002783-main.pdf:pdf},
journal = {Neuron},
number = {3},
pages = {483--492},
publisher = {Elsevier},
title = {{Fast odor learning improves reliability of odor responses in the locust antennal lobe.}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=15882647{\&}retmode=ref{\&}cmd=prlinks papers2://publication/doi/10.1016/j.neuron.2005.03.022},
volume = {46},
year = {2005}
}

@Misc{numpy,
title={ {NumPy} package for scientific computing},
url={https://www.numpy.org/},
}

@Misc{matplotlib,
title={ {Matplotlib} Python plotting library},
url={https://matplotlib.org/},
}

@article{Huxley1952,
abstract = {This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre (Hodgkin, Huxley {\&} Katz, 1952; Hodgkin {\&} Huxley, 1952 a-c). Its general object is to discu the results of the preceding papers (Part I), to put them into mathematical form (Part II) and to show that they will account for con- duction and excitation in quantitative terms (Part III). PART},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Huxley, A. L. and Hodgkin, A. F.},
journal = {Journal of Physiology},
title = {{Quantitative description of nerve current}},
year = {1952}
}

@article{Hodgkin1976,
author = {Hodgkin, A L},
file = {:Users/collins/Documents/watchFolderMendeley/jphysiol.1976.sp011620.pdf:pdf},
journal = {Journal of Physiology},
pages = {1--21},
title = {{Chance and design in electrophysiology: An informal account of certain experiments on nerve carried out between 1934 and 1952}},
volume = {263},
year = {1976}
}

@inbook{kreyszig1983,
author = {Kreyszig, Erwin},
title = {Advanced Engineering Mathematics},
booktitle = {â€¢},
year = {1983},
edition = {5},
publisher = {Wiley Eastern},
chapter = {21},
pages = {830-870},
}

@book{Dayan2005,
abstract = {Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory. The book is divided into three parts. Part I. discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.},
author = {Dayan, Peter and Abbott, Larry F.},
doi = {10.1017/CBO9781107415324.004},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
publisher = {MIT Press},
title = {{Theoretical Neuroscience - Computational and Mathematical Modeling of Neural Systems}},
year = {2005}
}

@book{Johnston1995,
abstract = {With simulations and illustrations by Richard GrayProblem solving is an indispensable part of learning a quantitative science such as neurophysiology. This text for graduate and advanced undergraduate students in neuroscience, physiology, biophysics, and computational neuroscience provides comprehensive, mathematically sophisticated descriptions of modern principles of cellular neurophysiology. It is the only neurophysiology text that gives detailed derivations of equations, worked examples, and homework problem sets (with complete answers).Developed from notes for the course that the authors have taught since 1983, Foundations of Cellular Neurophysiology covers cellular neurophysiology (also some material at the molecular and systems levels) from its physical and mathematical foundations in a way that is far more rigorous than other commonly used texts in this area.},
author = {Johnston, D. and Wu, S. M.S.},
booktitle = {MIT Press},
doi = {10.1097/00004691-199703000-00009},
isbn = {0262100533},
issn = {0277-2116},
pmid = {3944734},
publisher = {MIT Press},
title = {{Foundations of cellular neurophysiology}},
year = {1995}
}

@Misc{gerstnerMOOC,
title = {Neuronal dynamics},
OPTauthor = {Gerstner, Wulfram},
url = {https://www.edx.org/course/neuronal-dynamics},
}

@Misc{compneuroMOOC,
title = {Computational Neuroscience},
OPTauthor = {Rao, Rajesh P. N. and Fairhall, Adrienne},
url = {https://www.coursera.org/learn/computational-neuroscience},
}

@book{Gerstner2014,
abstract = {{\textcopyright} Cambridge University Press 2014. What happens in our brain when we make a decision? What triggers a neuron to send out a signal? What is the neural code? This textbook for advanced undergraduate and beginning graduate students provides a thorough and up-to-date introduction to the fields of computational and theoretical neuroscience. It covers classical topics, including the Hodgkin-Huxley equations and Hopfield model, as well as modern developments in the field such as Generalized Linear Models and decision theory. Concepts are introduced using clear step-by-step explanations suitable for readers with only a basic knowledge of differential equations and probabilities, and are richly illustrated by figures and worked-out examples. End-of-chapter summaries and classroom-tested exercises make the book ideal for courses or for self-study. The authors also give pointers to the literature and an extensive bibliography, which will prove invaluable to readers interested in further study.},
author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
doi = {10.1017/CBO9781107447615},
isbn = {9781107447615},
publisher = {Cambridge University Press},
title = {{Neuronal dynamics: From single neurons to networks and models of cognition}},
year = {2014}
}

@article{Destexhe1994,
abstract = {Markov kinetic models were used to synthesize a complete description of synaptic transmission, including opening of voltage-dependent channels in the presynaptic terminal, release of neurotransmitter, gating of postsynaptic receptors, and activation of second-messenger systems. These kinetic schemes provide a more general framework for modeling ion channels than the Hodgkin-Huxley formalism, supporting a continuous spectrum of descriptions ranging from the very simple and computationally efficient to the highly complex and biophysically precise. Examples are given of simple kinetic schemes based on fits to experimental data that capture the essential properties of voltage-gated, synaptic and neuromodulatory currents. The Markov formalism allows the dynamics of ionic currents to be considered naturally in the larger context of biochemical signal transduction. This framework can facilitate the integration of a wide range of experimental data and promote consistent theoretical analysis of neural mechanisms from molecular interactions to network computations.},
author = {Destexhe, Alain and Mainen, Zachary F. and Sejnowski, Terrence J.},
doi = {10.1007/BF00961734},
isbn = {0929-5313 (Print)$\backslash$r0929-5313 (Linking)},
issn = {09295313},
journal = {Journal of Computational Neuroscience},
number = {3},
pages = {195--230},
pmid = {8792231},
title = {{Synthesis of models for excitable membranes, synaptic transmission and neuromodulation using a common kinetic formalism}},
volume = {1},
year = {1994}
}
