
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Day 4: Neurons and Networks &#8212; PSST: Parallelised Scalable Simulations in TensorFlow</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/PSST-favicon-32x32.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Day 5: Optimal Mind Control" href="../Day%205%20Optimal%20Mind%20Control/Day%205.html" />
    <link rel="prev" title="Day 3: Cells in Silicon" href="../Day%203%20Cells%20in%20Silicon/Day%203.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/PSST.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">PSST: Parallelised Scalable Simulations in TensorFlow</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../README.html">
   PSST … It’s well Documented!
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README.html">
   Day 0 : Introduction to the Tutorials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Day%201%20Of%20Numerical%20Integration%20and%20Python/Day%201.html">
   Day 1: Of Numerical Integration and Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Day%202%20Let%20the%20Tensors%20Flow/Day%202.html">
   Day 2: Let the Tensors Flow!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Day%203%20Cells%20in%20Silicon/Day%203.html">
   Day 3: Cells in Silicon
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Day 4: Neurons and Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Day%205%20Optimal%20Mind%20Control/Day%205.html">
   Day 5: Optimal Mind Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Example%20Implementation%20Locust%20AL/Example.html">
   Day 6: (Example Implementation) Into the Mind of a Locust
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Optional%20Material/Distributed%20TensorFlow/Distributed%20TensorFlow.html">
   Day 7: (Optional) Distributed Computing with TensorFlow
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/Tutorial/Day 4 Neurons and Networks/Day 4.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/neurorishika/PSST"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/neurorishika/PSST/issues/new?title=Issue%20on%20page%20%2FTutorial/Day 4 Neurons and Networks/Day 4.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Day 4: Neurons and Networks
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-do-we-model-synapses">
     How do we model Synapses?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#modelling-synapses">
       Modelling Synapses
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#iterating-over-conditionals-in-tensorflow">
       Iterating over conditionals in TensorFlow
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#redesigning-the-generalized-tensorflow-integrator">
       Redesigning the Generalized TensorFlow Integrator
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implementing-a-network-of-hodgkin-huxley-neurons">
       Implementing a network of Hodgkin-Huxley Neurons
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#synaptic-memory-management">
       Synaptic Memory Management
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-the-connectivity">
       Defining the Connectivity
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-firing-thresholds">
       Defining Firing Thresholds
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-input-current-as-function-of-time">
       Defining Input Current as function of Time
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-the-connectivity-matrix-and-calculating-currents">
       Defining the connectivity matrix and calculating currents
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#steps-to-calculate-the-synaptic-currents">
         Steps to calculate the Synaptic Currents
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#updating-synaptic-variables">
       Updating Synaptic Variables
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#steps-to-calculate-t">
         Steps to calculate [T]
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#updaing-the-gating-variable-and-the-initial-conditions">
       Updaing the Gating Variable and the Initial Conditions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#creating-the-current-input-and-run-the-simulation">
       Creating the Current Input and Run the Simulation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#visualizing-and-interpreting-the-output">
       Visualizing and Interpreting the Output
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Day 4: Neurons and Networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Day 4: Neurons and Networks
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-do-we-model-synapses">
     How do we model Synapses?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#modelling-synapses">
       Modelling Synapses
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#iterating-over-conditionals-in-tensorflow">
       Iterating over conditionals in TensorFlow
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#redesigning-the-generalized-tensorflow-integrator">
       Redesigning the Generalized TensorFlow Integrator
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implementing-a-network-of-hodgkin-huxley-neurons">
       Implementing a network of Hodgkin-Huxley Neurons
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#synaptic-memory-management">
       Synaptic Memory Management
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-the-connectivity">
       Defining the Connectivity
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-firing-thresholds">
       Defining Firing Thresholds
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-input-current-as-function-of-time">
       Defining Input Current as function of Time
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-the-connectivity-matrix-and-calculating-currents">
       Defining the connectivity matrix and calculating currents
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#steps-to-calculate-the-synaptic-currents">
         Steps to calculate the Synaptic Currents
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#updating-synaptic-variables">
       Updating Synaptic Variables
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#steps-to-calculate-t">
         Steps to calculate [T]
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#updaing-the-gating-variable-and-the-initial-conditions">
       Updaing the Gating Variable and the Initial Conditions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#creating-the-current-input-and-run-the-simulation">
       Creating the Current Input and Run the Simulation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#visualizing-and-interpreting-the-output">
       Visualizing and Interpreting the Output
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><a href="https://colab.research.google.com/github/neurorishika/PSST/blob/master/Tutorial/Day%204%20Neurons%20and%20Networks/Day%204.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Day%204%20Neurons%20and%20Networks/Day%204.ipynb" target="_parent"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg" /></a></p>
<div class="section" id="day-4-neurons-and-networks">
<h1>Day 4: Neurons and Networks<a class="headerlink" href="#day-4-neurons-and-networks" title="Permalink to this headline">¶</a></h1>
<p>Welcome to Day 4! In this section we simulate a network of neurons interacting via synapses.</p>
<div class="section" id="how-do-we-model-synapses">
<h2>How do we model Synapses?<a class="headerlink" href="#how-do-we-model-synapses" title="Permalink to this headline">¶</a></h2>
<p>Each synapse is defined by its own set of state variables and differential equations governing their temporal evolution. There are different kinds of synapses - electrical and chemical synapses. Electrical synapses are essentially physical conduits that allow the flow of ions across connected neurons. Chemical synapses are more common in the brain and are more complex than electrical synapses. When an action potential arrives at the axon terminal, it leads to the opening of voltage-gated calcium channels. The incoming calcium triggers neurotransmitter filled vesicles to fuse with the axon terminal membrane and release their cargo into the synaptic cleft. The neurotransmitters diffuse across the cleft and open (or close) ion channels on the post-synaptic neuron. This can cause a depolarization (increase in potential across the post-synaptic neuron’s membrane) that makes it easier for the neuron to spike or it can inhibit the neuron and have the opposite effect. In some cases these effects are fast and direct - a neurotransmitter binds to a receptor in the post-synaptic site that causes an influx or efflux of ions and leads to a change in the membrane potential. The effect of a synapse can also be indirect such that neurotransmitters invoke a second messenger cascade that eventually leads to the opening or closing of ion channels in the post-synaptic neuron. Here we model fast excitatory and inhibitory chemical synapses.The network of interactions between neurons will be described by a connectivity matrix. Different connectivity matrices describe the interactions due to different types of synapses.</p>
<div class="section" id="modelling-synapses">
<h3>Modelling Synapses<a class="headerlink" href="#modelling-synapses" title="Permalink to this headline">¶</a></h3>
<p>The synaptic current (<span class="math notranslate nohighlight">\(I_{syn}\)</span>) depends on the difference between the reversal potential (<span class="math notranslate nohighlight">\(E_{syn}\)</span>) and the value of the membrane potential (<span class="math notranslate nohighlight">\(u\)</span>). The synaptic current due to neurotransmitter release into the synaptic cleft following an action potential is given by,
$<span class="math notranslate nohighlight">\(I_{syn}(t)=g_{syn}(t)(u(t)−E_{syn})\)</span>$</p>
<p>When the transmitter binds to postsynaptic receptors it causes a transient change in the conductance <span class="math notranslate nohighlight">\(g_{syn}\)</span>. To capture the dynamics of <span class="math notranslate nohighlight">\(g_{syn}\)</span>, one models the system using a simple kinetic model where the receptors can be either in an open or a closed state~\cite{Destexhe1994}. The transition between the states is proportional to the concentration of the neurotransmitter <span class="math notranslate nohighlight">\([T]\)</span> in the cleft.
$<span class="math notranslate nohighlight">\( \mathrm{C}\underset{\beta}{\stackrel{\alpha[T]}{\rightleftharpoons}} \mathrm{O} \)</span>$</p>
<p>This may be rewritten in the form of a differential equation.
$<span class="math notranslate nohighlight">\(\frac{d[O]}{dt}=\alpha[T](1−[O])−\beta[O]\)</span>$</p>
<p>We can now describe the synaptic conductance <span class="math notranslate nohighlight">\(g_{syn}(t)=g_{max}[O]\)</span>, in terms of the maximal conductance <span class="math notranslate nohighlight">\(g_{max}\)</span> and a gating variable <span class="math notranslate nohighlight">\([O]\)</span>, where <span class="math notranslate nohighlight">\([O](t)\)</span> is the fraction of open synaptic channels. <span class="math notranslate nohighlight">\(\alpha\)</span> is the binding constant, <span class="math notranslate nohighlight">\(\beta\)</span> the unbinding constant and <span class="math notranslate nohighlight">\((1−[O])\)</span> the fraction of closed channels where the neurotransmitter can bind. The functional form of T depends on the nature of the synapse.
For cholinergic excitatory synapses, <span class="math notranslate nohighlight">\([T]\)</span> is given by,</p>
<div class="math notranslate nohighlight">
\[[T]_{ach} = A\ \Theta(t_{max}+t_{fire}+t_{delay}-t)\ \Theta(t-t_{fire}-t_{delay})\]</div>
<p>where, <span class="math notranslate nohighlight">\(\Theta (x)\)</span> is the Heaviside step function, <span class="math notranslate nohighlight">\(t_{fire}\)</span> is the time of the last presynaptic spike, <span class="math notranslate nohighlight">\(t_{delay}\)</span> is the time delay from the time of the last spike to its effect on the postsynaptic neuron and <span class="math notranslate nohighlight">\(t_{max}\)</span> is the duration after the spike during which the transmitter remains in the synaptic cleft.
For Fast GABAergic inhibitory synapses, we used the following equation,</p>
<div class="math notranslate nohighlight">
\[[T]_{gaba} = \frac{1}{1+e^{-\frac{V(t-t_{fire}-t_{delay})-V_0}{\sigma}}}\]</div>
<p>Note that in order to solve this equation, we need to determine the time when the presynaptic neuron fired (<span class="math notranslate nohighlight">\(t_{fire}\)</span>). To account for these synaptic interactions between neurons we need to modify the RK4 integrator developed to simulate multiple independent Hodgkin-Huxley neurons.</p>
</div>
<div class="section" id="iterating-over-conditionals-in-tensorflow">
<h3>Iterating over conditionals in TensorFlow<a class="headerlink" href="#iterating-over-conditionals-in-tensorflow" title="Permalink to this headline">¶</a></h3>
<p>In this section we modify the integrator that we coded on Day 2 to account for interactions between neurons. This will require an additional variable that stores the time elapsed from the last presynaptic spike to calculate the  equations. In the modified code we will use the TensorFlow function <span class="math notranslate nohighlight">\(\texttt{tf.where()}\)</span> to efficiently assign the indices of neurons that have spiked and those that have not spiked at each time point. To understand the usage and function of <span class="math notranslate nohighlight">\(\texttt{tf.where()}\)</span>, consider the following example. Say, you have a array <span class="math notranslate nohighlight">\(\texttt{x}\)</span> of 10 random numbers between 0 and 1. You want the output of the code to be another array of the same size as <span class="math notranslate nohighlight">\(\texttt{x}\)</span> such that the elements of the array are either -10 or 10 depending on whether the corresponding element in <span class="math notranslate nohighlight">\(\texttt{x}\)</span> is less or greater than 0.5. The function  <span class="math notranslate nohighlight">\(\texttt{tf.where(cond,a,b)}\)</span> outputs an array with elements from <span class="math notranslate nohighlight">\(\texttt{a}\)</span> if the condition <span class="math notranslate nohighlight">\(\texttt{cond}\)</span> is <span class="math notranslate nohighlight">\(\texttt{True}\)</span> and from <span class="math notranslate nohighlight">\(\texttt{b}\)</span> if <span class="math notranslate nohighlight">\(\texttt{cond}\)</span> is <span class="math notranslate nohighlight">\(\texttt{False}\)</span>. See the example code below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">tensorflow.compat.v1</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span>

<span class="c1"># create the Tensor with the random variables</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,)),</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="c1"># a list of 10s to select from if true</span>
<span class="n">if_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,)),</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="c1"># a list of -10s to select from if false</span>
<span class="n">if_false</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,)),</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="c1"># perform the conditional masking</span>
<span class="n">selection</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span><span class="n">if_true</span><span class="p">,</span><span class="n">if_false</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">x_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">selection_out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">selection</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x_out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">selection_out</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.67409821 0.45097017 0.70497614 0.82002797 0.94004653 0.88966049
 0.33526257 0.16079188 0.8861437  0.13353322]
[ 10. -10.  10.  10.  10.  10. -10. -10.  10. -10.]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="redesigning-the-generalized-tensorflow-integrator">
<h3>Redesigning the Generalized TensorFlow Integrator<a class="headerlink" href="#redesigning-the-generalized-tensorflow-integrator" title="Permalink to this headline">¶</a></h3>
<p>In order to determine whether a particular neuron fired, we introduce a new variable fire_t that stores the time of the last spike for each neuron.</p>
<p>We modify the code as follows:</p>
<ol class="simple">
<li><p>The Integrator class that we defined earlier now requires two more properties as input, namely, the number of neurons (<span class="math notranslate nohighlight">\(n\)</span>) and the firing threshold (<span class="math notranslate nohighlight">\(F_b\)</span>) of each of these neurons. We provide these inputs as arguments to the Integrator class.</p></li>
<li><p>The state vector will now have an additional set of <span class="math notranslate nohighlight">\(n\)</span> variables representing the firing times. These will not be updated by the step function (<span class="math notranslate nohighlight">\(\_step\_func\)</span>).</p></li>
<li><p>Inside the Integrator class, we have access to the values of the state variable and the change in the state variable since the last iteration. We use this to check if the voltages have crossed the firing threshold. The convention followed in this code is, the first <span class="math notranslate nohighlight">\(n\)</span> elements of the state vector are the membrane voltages while the last <span class="math notranslate nohighlight">\(n\)</span> elements are the time from the last spike for each of the neurons.</p></li>
<li><p>The differential update function, <span class="math notranslate nohighlight">\(step\_func\)</span>, takes all except the last <span class="math notranslate nohighlight">\(n\)</span> values of the state variable and updates it according to the differential equations specified in <span class="math notranslate nohighlight">\(func\)</span>. The last <span class="math notranslate nohighlight">\(n\)</span> variables are updated separately in <span class="math notranslate nohighlight">\(scan\_func\)</span>. It checks if any neuron has crossed its firing threshold and updates the variable <span class="math notranslate nohighlight">\(fire\_t\)</span> of the appropriate neurons with the current time.</p></li>
</ol>
<p>The modifications to the RK4 code implemented earlier is shown below,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">tensorflow.compat.v1</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span>

<span class="o">%</span><span class="k">matplotlib</span> inline


<span class="k">def</span> <span class="nf">tf_check_type</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y0</span><span class="p">):</span> <span class="c1"># Ensure Input is Correct</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function checks the type of the input to ensure that it is a floating point number.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">y0</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Error: y0 and t must be floating point numbers.&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">_Tf_Integrator</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class implements the Runge-Kutta 4th order method in TensorFlow with last Spike Time Calculation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_</span><span class="p">,</span><span class="n">F_b</span><span class="p">):</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function initializes the class with the number of neurons and the firing threshold.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_ : int</span>
<span class="sd">            Number of neurons in the system.</span>
<span class="sd">        F_b : list or np.array of floats</span>
<span class="sd">            Firing threshold of the neurons.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># class constructor to get inputs for number of neurons and firing thresholds</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_</span> <span class="o">=</span> <span class="n">n_</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">F_b</span> <span class="o">=</span> <span class="n">F_b</span>
    
    <span class="k">def</span> <span class="nf">integrate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function integrates a function func using the Runge-Kutta 4th order method in TensorFlow with last Spike Time Calculation.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        func: function</span>
<span class="sd">            The function to be integrated.</span>
<span class="sd">        y0: float</span>
<span class="sd">            The initial condition.</span>
<span class="sd">        t: numpy array</span>
<span class="sd">            The time array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">time_delta_grid</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">t</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># define the time step at each point</span>
        
        <span class="k">def</span> <span class="nf">scan_func</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t_dt</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            This function performs the integration step and last spike time calculation.</span>
<span class="sd">            </span>
<span class="sd">            Parameters:</span>
<span class="sd">            -----------</span>
<span class="sd">            y: float</span>
<span class="sd">                The value of y at which the function is being evaluated.</span>
<span class="sd">            t_dt: (float, float)</span>
<span class="sd">                The time point and time step at which the function is being evaluated.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="c1"># recall the necessary variables</span>
            <span class="n">n_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_</span> <span class="c1"># number of neurons</span>
            <span class="n">F_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">F_b</span> <span class="c1"># firing threshold</span>
            
            <span class="n">t</span><span class="p">,</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">t_dt</span> <span class="c1"># unpack the time point and time step</span>
            
            <span class="c1"># Differential updation</span>
            <span class="n">dy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_func</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="n">dt</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1"># Make code more modular.</span>
            <span class="n">dy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="c1"># Failsafe</span>
           
            <span class="n">out</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="n">dy</span> <span class="c1"># the result after differential updation</span>
        
            <span class="c1"># Conditional to use specialized Integrator vs Normal Integrator (n=0)</span>
            <span class="k">if</span> <span class="n">n_</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
                
                <span class="c1"># Extract the last n variables for fire times</span>
                <span class="n">fire_t</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="n">n_</span><span class="p">:]</span> 
                
                <span class="c1"># Value of change in firing times if neuron didnt fire = 0</span>
                <span class="n">l</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">fire_t</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">fire_t</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> 
                
                <span class="c1"># Value of change in firing times if neuron fired = Current Time - Last Fire Time</span>
                <span class="n">l_</span> <span class="o">=</span> <span class="n">t</span><span class="o">-</span><span class="n">fire_t</span> 
                
                <span class="c1"># Check if Voltage is initially less than Firing Threshold</span>
                <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">y</span><span class="p">[:</span><span class="n">n_</span><span class="p">],</span><span class="n">F_b</span><span class="p">)</span>              
                <span class="c1"># Check if Voltage is more than Firing Threshold after updation</span>
                <span class="n">z_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">out</span><span class="p">[:</span><span class="n">n_</span><span class="p">],</span><span class="n">F_b</span><span class="p">)</span>  
                
                <span class="n">df</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">z_</span><span class="p">),</span><span class="n">l_</span><span class="p">,</span><span class="n">l</span><span class="p">)</span> 
                
                <span class="n">fire_t_</span> <span class="o">=</span> <span class="n">fire_t</span><span class="o">+</span><span class="n">df</span> <span class="c1"># Update firing time </span>
                
                <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">out</span><span class="p">[:</span><span class="o">-</span><span class="n">n_</span><span class="p">],</span><span class="n">fire_t_</span><span class="p">],</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># Remove and Update the last n variables with new last spike times</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">out</span>
            
        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">scan_func</span><span class="p">,</span> <span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">time_delta_grid</span><span class="p">),</span><span class="n">y0</span><span class="p">)</span> <span class="c1"># Perform the integration</span>
        
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">y0</span><span class="p">],</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># Add the initial condition to the result and return</span>
    
    <span class="k">def</span> <span class="nf">_step_func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function determines the size of the step.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        func: function</span>
<span class="sd">            The function to be integrated.</span>
<span class="sd">        t: float</span>
<span class="sd">            The time point at which the function is being evaluated.</span>
<span class="sd">        dt: float</span>
<span class="sd">            The time step at which the function is being integrated.</span>
<span class="sd">        y: float</span>
<span class="sd">            The value of y at which the function is being evaluated.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">k1</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">half_step</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">dt_cast</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="c1"># Failsafe</span>

        <span class="n">k2</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">dt_cast</span> <span class="o">*</span> <span class="n">k1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">half_step</span><span class="p">)</span>
        <span class="n">k3</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">dt_cast</span> <span class="o">*</span> <span class="n">k2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">half_step</span><span class="p">)</span>
        <span class="n">k4</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">dt_cast</span> <span class="o">*</span> <span class="n">k3</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="n">dt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">([</span><span class="n">k1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k3</span><span class="p">,</span> <span class="n">k4</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="n">dt_cast</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span>
    

<span class="k">def</span> <span class="nf">odeint</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n_</span><span class="p">,</span> <span class="n">F_b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function integrates the function func using the modified Runge-Kutta 4th order method implemented in the _Tf_Integrator class</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    func: function</span>
<span class="sd">        The function to be integrated.</span>
<span class="sd">    y0: float</span>
<span class="sd">        The initial condition.</span>
<span class="sd">    t: numpy array</span>
<span class="sd">        The time array.</span>
<span class="sd">    n_: int</span>
<span class="sd">        Number of neurons in the system.</span>
<span class="sd">    F_b: list or np.array of floats</span>
<span class="sd">        Firing threshold of the neurons.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Ensure Input is in the form of TensorFlow Tensors</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">preferred_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;t&#39;</span><span class="p">)</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;y0&#39;</span><span class="p">)</span>
    <span class="n">tf_check_type</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span><span class="n">t</span><span class="p">)</span> <span class="c1"># Ensure Input is of the correct type</span>
    <span class="k">return</span> <span class="n">_Tf_Integrator</span><span class="p">(</span><span class="n">n_</span><span class="p">,</span> <span class="n">F_b</span><span class="p">)</span><span class="o">.</span><span class="n">integrate</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="n">y0</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="implementing-a-network-of-hodgkin-huxley-neurons">
<h3>Implementing a network of Hodgkin-Huxley Neurons<a class="headerlink" href="#implementing-a-network-of-hodgkin-huxley-neurons" title="Permalink to this headline">¶</a></h3>
<p>Recall, each Hodgkin Huxley Neuron in a network with <span class="math notranslate nohighlight">\(n\)</span> neurons has 4 dynamical variables <span class="math notranslate nohighlight">\(V\)</span>, <span class="math notranslate nohighlight">\( m\)</span>, <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(h\)</span>. Each of these variables were represented as <span class="math notranslate nohighlight">\(n\)</span>-dimensional vectors. Now we need to add some more state variables representing each synapse. The neuron receives excitatory and inhibitory inputs that are introduced as additional synaptic currents <span class="math notranslate nohighlight">\(I_{ach}\)</span> and <span class="math notranslate nohighlight">\(I_{GABA}\)</span>. The equation now reads,</p>
<div class="math notranslate nohighlight">
\[C_m\frac{dV}{dt} = I_{injected} - I_{Na} - I_K - I_L - I_{ach} - I_{gaba}\]</div>
<p>For each synapse, we have:</p>
<div class="math notranslate nohighlight">
\[\frac{d[O]_{ach/gaba}}{dt} = \alpha (1-[O]_{ach/gaba})[T]_{ach/gaba}-\beta[O]_{ach/gaba}\]</div>
<div class="math notranslate nohighlight">
\[I_{ach/gaba}(t)=g_{max}[O]_{ach/gaba}(V−E_{ach/gaba})\]</div>
</div>
<div class="section" id="synaptic-memory-management">
<h3>Synaptic Memory Management<a class="headerlink" href="#synaptic-memory-management" title="Permalink to this headline">¶</a></h3>
<p>As discussed earlier, there are atmost <span class="math notranslate nohighlight">\(n^2\)</span> synapses of each type but at a time, unless the network is fully connected/very dense, mostly we need a very small subset of these synapses. We could, in principle, calculate the dynamics of all <span class="math notranslate nohighlight">\(n^2\)</span> but it would be pointless. So have to devise a matrix based sparse-dense coding system for evaluating the dynamics of these variables and also using their values. This will reduce memory usage and minimize number of calculations at the cost of time for encoding and decoding into dense data from sparse data and vice versa. This is why we use a matrix approach, so that tensorflow can speed up the process.</p>
</div>
<div class="section" id="defining-the-connectivity">
<h3>Defining the Connectivity<a class="headerlink" href="#defining-the-connectivity" title="Permalink to this headline">¶</a></h3>
<p>In a network with n neurons, there are at most <span class="math notranslate nohighlight">\(n^2\)</span> synapses of each type. The actual number may be much smaller. To illustrate the details of the implementation, consider the following three neuron network. Let <span class="math notranslate nohighlight">\(X_1\)</span> be an excitatory neuron that forms a cholinergic synapse, <span class="math notranslate nohighlight">\(X_2\)</span> an inhibitory neuron that extends a GABAergic synapse onto <span class="math notranslate nohighlight">\(X_3\)</span>. The network has the form: <span class="math notranslate nohighlight">\(X_1 \rightarrow X_2 \rightarrow X_3\)</span>. In defining the connectivity matrix for each synapse type, we set a convention where the presynaptic neurons are indexed by the column number, and the postsynaptic neurons by the row number. Let <span class="math notranslate nohighlight">\(X_1\)</span>, <span class="math notranslate nohighlight">\(X_2\)</span>, <span class="math notranslate nohighlight">\(X_3\)</span> be indexed as 0, 1 and 2 respectively. The excitatory connectivity matrix takes the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}Ach_{n\times n}=
\begin{bmatrix}
0&amp;0&amp;0\\
1&amp;0&amp;0\\
0&amp;0&amp;0\\
\end{bmatrix}
\end{split}\]</div>
<p>Similarly, the inhibitory connectivity matrix becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}GABA_{n\times n}=
\begin{bmatrix}
0&amp;0&amp;0\\
0&amp;0&amp;0\\
0&amp;1&amp;0\\
\end{bmatrix}
\end{split}\]</div>
<p>In the following code we specify the parameters of the synapse. The number of synapses of each type are determined by adding up all the elements of the connectivity matrix. Other parameters are specified as vectors with values for each of the synapses.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_n</span> <span class="o">=</span> <span class="mi">3</span>                             <span class="c1"># number of simultaneous neurons to simulate</span>

<span class="c1"># Acetylcholine</span>

<span class="n">ach_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_n</span><span class="p">,</span><span class="n">n_n</span><span class="p">))</span>       <span class="c1"># Ach Synapse Connectivity Matrix</span>
<span class="n">ach_mat</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>

<span class="c1">## PARAMETERS FOR ACETYLCHLOLINE SYNAPSES ##</span>

<span class="n">n_ach</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ach_mat</span><span class="p">))</span>        <span class="c1"># Number of Acetylcholine (Ach) Synapses </span>
<span class="n">alp_ach</span> <span class="o">=</span> <span class="p">[</span><span class="mf">10.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_ach</span>              <span class="c1"># Alpha for Ach Synapse</span>
<span class="n">bet_ach</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">]</span><span class="o">*</span><span class="n">n_ach</span>               <span class="c1"># Beta for Ach Synapse</span>
<span class="n">t_max</span> <span class="o">=</span> <span class="mf">0.3</span>                         <span class="c1"># Maximum Time for Synapse</span>
<span class="n">t_delay</span> <span class="o">=</span> <span class="mi">0</span>                         <span class="c1"># Axonal Transmission Delay</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>                       <span class="c1"># Synaptic Response Strength</span>
<span class="n">g_ach</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.35</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>                  <span class="c1"># Ach Conductance</span>
<span class="n">E_ach</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>                   <span class="c1"># Ach Potential</span>

<span class="c1"># GABAa</span>

<span class="n">gaba_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_n</span><span class="p">,</span><span class="n">n_n</span><span class="p">))</span>      <span class="c1"># GABAa Synapse Connectivity Matrix</span>
<span class="n">gaba_mat</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">## PARAMETERS FOR GABAa SYNAPSES ##</span>

<span class="n">n_gaba</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gaba_mat</span><span class="p">))</span>      <span class="c1"># Number of GABAa Synapses</span>
<span class="n">alp_gaba</span> <span class="o">=</span> <span class="p">[</span><span class="mf">10.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_gaba</span>            <span class="c1"># Alpha for GABAa Synapse</span>
<span class="n">bet_gaba</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.16</span><span class="p">]</span><span class="o">*</span><span class="n">n_gaba</span>            <span class="c1"># Beta for GABAa Synapse</span>
<span class="n">V0</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">20.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>                    <span class="c1"># Decay Potential</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>                   <span class="c1"># Decay Time Constant</span>
<span class="n">g_gaba</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>                  <span class="c1"># fGABA Conductance</span>
<span class="n">E_gaba</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">70.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>                <span class="c1"># fGABA Potential</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="defining-firing-thresholds">
<h3>Defining Firing Thresholds<a class="headerlink" href="#defining-firing-thresholds" title="Permalink to this headline">¶</a></h3>
<p>We shall also define a list that stores the firing threshold for every neuron.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Storing Firing Thresholds ##</span>
<span class="n">F_b</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>                      <span class="c1"># Fire threshold</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="defining-input-current-as-function-of-time">
<h3>Defining Input Current as function of Time<a class="headerlink" href="#defining-input-current-as-function-of-time" title="Permalink to this headline">¶</a></h3>
<p>We can store our input to each neuron as a <span class="math notranslate nohighlight">\(n\times timesteps\)</span> matrix, say current_input, and extract the input at each time point during dynamical update step using a function which we shall call I_inj_t(t).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">I_inj_t</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function returns the external current to be injected into the network at any time step from the current_input matrix.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    t: float</span>
<span class="sd">        The time at which the current injection is being performed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Turn indices to integer and extract from matrix</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">t</span><span class="o">/</span><span class="n">epsilon</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">current_input</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)[</span><span class="n">index</span><span class="p">]</span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="defining-the-connectivity-matrix-and-calculating-currents">
<h3>Defining the connectivity matrix and calculating currents<a class="headerlink" href="#defining-the-connectivity-matrix-and-calculating-currents" title="Permalink to this headline">¶</a></h3>
<p>For updating the dynamics of synapses, we need only as many variables as the number of synapses <span class="math notranslate nohighlight">\(\times\)</span> number of equations required for each synapse. Here our synapse models require only one dynamical variable, the fraction of open channels <span class="math notranslate nohighlight">\([O]\)</span>, that we store as an <span class="math notranslate nohighlight">\(k\)</span>—dimensional vector, where <span class="math notranslate nohighlight">\(k\)</span> is the number of synapses. There are two instances where the <span class="math notranslate nohighlight">\([O]\)</span> vector is used. First, to solve the equation for <span class="math notranslate nohighlight">\([o]\)</span> and second, to calculate the synaptic current given by,</p>
<div class="math notranslate nohighlight">
\[I_{syn} = \sum_{presynaptic} g_{syn}[O](V-E_{syn})\]</div>
<p>The best way to represent this calculation is to use the connectivity matrix <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> for the synapses and the open fraction vector <span class="math notranslate nohighlight">\(\vec{[O]}\)</span> to create an open fraction matrix <span class="math notranslate nohighlight">\(\mathbf{O}\)</span> and perform the following computations.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{C}=
\begin{bmatrix}
0&amp;1&amp;...&amp;0\\
0&amp;0&amp;...&amp;1\\
...&amp;...&amp;...&amp;1\\
1&amp;0&amp;0&amp;0
\end{bmatrix}\ \ \ \ \ \ \ \ \vec{[O]}=[O_1,O_2...O_k]\ \ \ \ \ \ \ \ 
\mathbf{O}=
\begin{bmatrix}
0&amp;O_1&amp;...&amp;0\\
0&amp;0&amp;...&amp;O_a\\
...&amp;...&amp;...&amp;O_b\\
O_k&amp;0&amp;0&amp;0
\end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\vec{[I_{syn}]}=\sum_{columns}\mathbf{O}\diamond(\vec{g}_{syn}\odot(\vec{V}-\vec{E}_{syn}))\]</div>
<p>where <span class="math notranslate nohighlight">\(\diamond\)</span> is columnwise multiplication and <span class="math notranslate nohighlight">\(\odot\)</span> is elementwise multiplication. <span class="math notranslate nohighlight">\(\vec{[I_{syn}]}\)</span> is now the total synaptic current input to the each of the neurons.</p>
<div class="section" id="steps-to-calculate-the-synaptic-currents">
<h4>Steps to calculate the Synaptic Currents<a class="headerlink" href="#steps-to-calculate-the-synaptic-currents" title="Permalink to this headline">¶</a></h4>
<ol class="simple">
<li><p>Firstly we need to convert from the sparse <span class="math notranslate nohighlight">\([O]\)</span> vector to the dense <span class="math notranslate nohighlight">\(\mathbf{O}\)</span> matrix. TensorFlow does not allow to make changes to a defined tensor directly, thus we create a <span class="math notranslate nohighlight">\(n^{2}\)</span> vector TensorFlow variable o_ which we will later reshape to a <span class="math notranslate nohighlight">\(n\times n\)</span> matrix.</p></li>
<li><p>We then flatten the synaptic connectivity matrix and find the indices where there is a connection. For this we use the boolean mask function to choose the correct k (total number of synapses) indices from the range <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(n^2\)</span> and store in the variable ind.</p></li>
<li><p>Using the scatter_update function of TensorFlow, we fill the correct indices of the variable o_ that we created with the values of open fraction from the <span class="math notranslate nohighlight">\([O]\)</span> vector.</p></li>
<li><p>We now reshape the vector as a <span class="math notranslate nohighlight">\(n\times n\)</span> matrix. Since python stores matrices as array of arrays, with each row as an inner array, for performing columnswise multiplication, the easiest way is to tranpose the matrix, so that each column is the inner array, perform element wise multiplication with each inner array and apply transpose again.</p></li>
<li><p>Finally using reduce_sum, we sum over the columns to get our <span class="math notranslate nohighlight">\(I_{syn}\)</span> vector.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Acetylcholine Synaptic Current ##</span>

<span class="k">def</span> <span class="nf">I_ach</span><span class="p">(</span><span class="n">o</span><span class="p">,</span><span class="n">V</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function returns the synaptic current for the Acetylcholine (Ach) synapses for each neuron.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    o: float</span>
<span class="sd">        The fraction of open acetylcholine channels for each synapse.</span>
<span class="sd">    V: float</span>
<span class="sd">        The membrane potential of the postsynaptic neuron.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">o_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="c1"># Initialize the flattened matrix to store the synaptic open fractions</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">n_n</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span><span class="n">ach_mat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Get the indices of the synapses that exist</span>
    <span class="n">o_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tensor_scatter_nd_update</span><span class="p">(</span><span class="n">o_</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ind</span><span class="p">,[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span><span class="n">o</span><span class="p">)</span> <span class="c1"># Update the flattened open fraction matrix</span>
    <span class="n">o_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">o_</span><span class="p">,(</span><span class="n">n_n</span><span class="p">,</span><span class="n">n_n</span><span class="p">)))</span> <span class="c1"># Reshape and Transpose the matrix to be able to multiply it with the conductance matrix</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="n">o_</span><span class="o">*</span><span class="p">(</span><span class="n">V</span><span class="o">-</span><span class="n">E_ach</span><span class="p">))</span><span class="o">*</span><span class="n">g_ach</span><span class="p">),</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Calculate the synaptic current</span>

<span class="c1">## GABAa Synaptic Current ##</span>

<span class="k">def</span> <span class="nf">I_gaba</span><span class="p">(</span><span class="n">o</span><span class="p">,</span><span class="n">V</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function returns the synaptic current for the GABA synapses for each neuron.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    o: float</span>
<span class="sd">        The fraction of open GABA channels for each synapse.</span>
<span class="sd">    V: float</span>
<span class="sd">        The membrane potential of the postsynaptic neuron.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">o_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="c1"># Initialize the flattened matrix to store the synaptic open fractions</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">n_n</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span><span class="n">gaba_mat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Get the indices of the synapses that exist</span>
    <span class="n">o_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tensor_scatter_nd_update</span><span class="p">(</span><span class="n">o_</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ind</span><span class="p">,[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span><span class="n">o</span><span class="p">)</span> <span class="c1"># Update the flattened open fraction matrix</span>
    <span class="n">o_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">o_</span><span class="p">,(</span><span class="n">n_n</span><span class="p">,</span><span class="n">n_n</span><span class="p">)))</span> <span class="c1"># Reshape and Transpose the matrix to be able to multiply it with the conductance matrix</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="n">o_</span><span class="o">*</span><span class="p">(</span><span class="n">V</span><span class="o">-</span><span class="n">E_gaba</span><span class="p">))</span><span class="o">*</span><span class="n">g_gaba</span><span class="p">),</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Calculate the synaptic current</span>

<span class="c1"># For TensorFlow v2.x, use tf.tensor_scatter_nd_update instead of </span>
<span class="c1"># tf.scatter_update and make o_ a tf.constant instead of a tf.variable</span>
<span class="c1"># Also ind must be reshaped to be a column matrix.</span>

<span class="c1">## Other Currents ##</span>

<span class="k">def</span> <span class="nf">I_K</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function determines the K-channel current.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    V: float</span>
<span class="sd">        The membrane potential.</span>
<span class="sd">    n: float </span>
<span class="sd">        The K-channel gating variable n.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">g_K</span>  <span class="o">*</span> <span class="n">n</span><span class="o">**</span><span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">E_K</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">I_Na</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function determines the Na-channel current.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    V: float</span>
<span class="sd">        The membrane potential.</span>
<span class="sd">    m: float</span>
<span class="sd">        The Na-channel gating variable m.</span>
<span class="sd">    h: float</span>
<span class="sd">        The Na-channel gating variable h.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">g_Na</span> <span class="o">*</span> <span class="n">m</span><span class="o">**</span><span class="mi">3</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">E_Na</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">I_L</span><span class="p">(</span><span class="n">V</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function determines the leak current.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    V: float</span>
<span class="sd">        The membrane potential.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">g_L</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">E_L</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="updating-synaptic-variables">
<h3>Updating Synaptic Variables<a class="headerlink" href="#updating-synaptic-variables" title="Permalink to this headline">¶</a></h3>
<p>To update the synapses we first calculate the values of the presynaptic activation function <span class="math notranslate nohighlight">\([T]\)</span> for both types of synapses. This function determines whether a neuron fired or not and is calculated for each neuron. The values of <span class="math notranslate nohighlight">\([T]\)</span> are then sent to the correct synapses in the form of a <span class="math notranslate nohighlight">\(k \times 1\)</span> vector. Recall:</p>
<div class="math notranslate nohighlight">
\[[T]_{ach} = A\ \Theta(t_{max}+t_{fire}+t_{delay}-t)\ \Theta(t-t_{fire}-t_{delay})\]</div>
<div class="math notranslate nohighlight">
\[[T]_{gaba} = \frac{1}{1+e^{-\frac{V(t)-V_0}{\sigma}}}\]</div>
<p>Once we calculate the values of <span class="math notranslate nohighlight">\([T]\)</span>-vector for both types of synapse, we need to redirect them to the correct synapses in a sparse <span class="math notranslate nohighlight">\(k \times 1\)</span> vector form.</p>
<div class="section" id="steps-to-calculate-t">
<h4>Steps to calculate [T]<a class="headerlink" href="#steps-to-calculate-t" title="Permalink to this headline">¶</a></h4>
<ol class="simple">
<li><p>To calculate <span class="math notranslate nohighlight">\([T]_{ach}\)</span>, use a boolean logical_and function to check is the current timepoint t is greater than the last fire time (fire_t) + delay (t_delay) and less than last fire time (fire_t) + delay (t_delay) + activation length (t_max) for each neuron as a vector. Use the result of these boolean operations to choose between zero or an constant A. This serves as the heaviside step function. For <span class="math notranslate nohighlight">\([T]_{gaba}\)</span>, simply use the V vector to determine T.</p></li>
<li><p>To determine the [T] vector, we follow as two step process. First we multiply each row of the connectivity matrices <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> with the respective <span class="math notranslate nohighlight">\([T]\)</span> vector to get a activation matrix <span class="math notranslate nohighlight">\(\mathbf{T}\)</span>, and then we just flatten <span class="math notranslate nohighlight">\(\mathbf{T}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> and, using tf.boolean_mask, remove all the zeros from <span class="math notranslate nohighlight">\(\mathbf{T}\)</span> to get a <span class="math notranslate nohighlight">\(k\times1\)</span> vector which now stores the presynaptic activation for each of the synapses where <span class="math notranslate nohighlight">\(k=n_{gaba}\)</span> or <span class="math notranslate nohighlight">\(n_{ach}\)</span></p></li>
<li><p>Calculate the differential change in the open fractions using the <span class="math notranslate nohighlight">\(k\times1\)</span> vector.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dXdt</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function determines the derivatives of the membrane voltage and gating variables for n_n neurons.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    X: float</span>
<span class="sd">        The state vector given by the [V1,V2,...,Vn_n,m1,m2,...,mn_n,h1,h2,...,hn_n,n1,n2,...,nn_n] where </span>
<span class="sd">            Vx is the membrane potential for neuron x</span>
<span class="sd">            mx is the Na-channel gating variable for neuron x </span>
<span class="sd">            hx is the Na-channel gating variable for neuron x</span>
<span class="sd">            nx is the K-channel gating variable for neuron x.</span>
<span class="sd">    t: float</span>
<span class="sd">        The time points at which the derivatives are being evaluated.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="o">*</span><span class="n">n_n</span><span class="p">]</span>       <span class="c1"># First n_n values are Membrane Voltage</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="o">*</span><span class="n">n_n</span><span class="p">:</span><span class="mi">2</span><span class="o">*</span><span class="n">n_n</span><span class="p">]</span>  <span class="c1"># Next n_n values are Sodium Activation Gating Variables</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">n_n</span><span class="p">:</span><span class="mi">3</span><span class="o">*</span><span class="n">n_n</span><span class="p">]</span>  <span class="c1"># Next n_n values are Sodium Inactivation Gating Variables</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">3</span><span class="o">*</span><span class="n">n_n</span><span class="p">:</span><span class="mi">4</span><span class="o">*</span><span class="n">n_n</span><span class="p">]</span>  <span class="c1"># Next n_n values are Potassium Gating Variables</span>
    <span class="n">o_ach</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">4</span><span class="o">*</span><span class="n">n_n</span> <span class="p">:</span> <span class="mi">4</span><span class="o">*</span><span class="n">n_n</span> <span class="o">+</span> <span class="n">n_ach</span><span class="p">]</span> <span class="c1"># Next n_ach values are Acetylcholine Synapse Open Fractions</span>
    <span class="n">o_gaba</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">4</span><span class="o">*</span><span class="n">n_n</span> <span class="o">+</span> <span class="n">n_ach</span> <span class="p">:</span> <span class="mi">4</span><span class="o">*</span><span class="n">n_n</span> <span class="o">+</span> <span class="n">n_ach</span> <span class="o">+</span> <span class="n">n_gaba</span><span class="p">]</span> <span class="c1"># Next n_gaba values are GABAa Synapse Open Fractions</span>
    <span class="n">fire_t</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="n">n_n</span><span class="p">:]</span>   <span class="c1"># Last n_n values are the last fire times as updated by the modified integrator</span>
    
    <span class="n">dVdt</span> <span class="o">=</span> <span class="p">(</span><span class="n">I_inj_t</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">-</span> <span class="n">I_Na</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">I_K</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">I_L</span><span class="p">(</span><span class="n">V</span><span class="p">)</span> <span class="o">-</span> <span class="n">I_ach</span><span class="p">(</span><span class="n">o_ach</span><span class="p">,</span><span class="n">V</span><span class="p">)</span> <span class="o">-</span> <span class="n">I_gaba</span><span class="p">(</span><span class="n">o_gaba</span><span class="p">,</span><span class="n">V</span><span class="p">))</span> <span class="o">/</span> <span class="n">C_m</span> <span class="c1"># The derivative of the membrane potential</span>
    
    <span class="c1">## Updation for gating variables ##</span>
    
    <span class="n">m0</span><span class="p">,</span><span class="n">tm</span><span class="p">,</span><span class="n">h0</span><span class="p">,</span><span class="n">th</span> <span class="o">=</span> <span class="n">Na_prop</span><span class="p">(</span><span class="n">V</span><span class="p">)</span> <span class="c1"># Calculate the dynamics of the Na-channel gating variables for all n_n neurons</span>
    <span class="n">n0</span><span class="p">,</span><span class="n">tn</span> <span class="o">=</span> <span class="n">K_prop</span><span class="p">(</span><span class="n">V</span><span class="p">)</span> <span class="c1"># Calculate the dynamics of the K-channel gating variables for all n_n neurons</span>

    <span class="n">dmdt</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">tm</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">m</span><span class="o">-</span><span class="n">m0</span><span class="p">)</span> <span class="c1"># The derivative of the Na-channel gating variable m for all n_n neurons</span>
    <span class="n">dhdt</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">th</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">h</span><span class="o">-</span><span class="n">h0</span><span class="p">)</span> <span class="c1"># The derivative of the Na-channel gating variable h for all n_n neurons</span>
    <span class="n">dndt</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">tn</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">n0</span><span class="p">)</span> <span class="c1"># The derivative of the K-channel gating variable n for all n_n neurons</span>
    
    <span class="c1">## Updation for o_ach ##</span>
    
    <span class="n">A_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="c1"># Get the synaptic response strengths of the pre-synaptic neurons</span>
    <span class="n">Z_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">A_</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="c1"># Create a zero matrix of the same size as A_</span>
    
    <span class="n">T_ach</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">fire_t</span><span class="o">+</span><span class="n">t_delay</span><span class="p">),</span><span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">fire_t</span><span class="o">+</span><span class="n">t_max</span><span class="o">+</span><span class="n">t_delay</span><span class="p">)),</span><span class="n">A_</span><span class="p">,</span><span class="n">Z_</span><span class="p">)</span>  <span class="c1"># Find which synapses would have received an presynaptic spike in the past window and assign them the corresponding synaptic response strength</span>
    <span class="n">T_ach</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">ach_mat</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">),</span><span class="n">T_ach</span><span class="p">)</span> <span class="c1"># Find the postsynaptic neurons that would have received an presynaptic spike in the past window</span>
    <span class="n">T_ach</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">T_ach</span><span class="p">,(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)),</span><span class="n">ach_mat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Get the pre-synaptic activation function for only the existing synapses</span>
    
    <span class="n">do_achdt</span> <span class="o">=</span> <span class="n">alp_ach</span><span class="o">*</span><span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">o_ach</span><span class="p">)</span><span class="o">*</span><span class="n">T_ach</span> <span class="o">-</span> <span class="n">bet_ach</span><span class="o">*</span><span class="n">o_ach</span>  <span class="c1"># Calculate the derivative of the open fraction of the acetylcholine synapses</span>
     
    <span class="c1">## Updation for o_gaba ##</span>
          
    <span class="n">T_gaba</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span><span class="o">+</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span><span class="o">-</span><span class="n">V0</span><span class="p">)</span><span class="o">/</span><span class="n">sigma</span><span class="p">))</span> <span class="c1"># Calculate the presynaptic activation function for all n_n neurons</span>
    <span class="n">T_gaba</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">gaba_mat</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">),</span><span class="n">T_gaba</span><span class="p">)</span> <span class="c1"># Find the postsynaptic neurons that would have received an presynaptic spike in the past window</span>
    <span class="n">T_gaba</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">T_gaba</span><span class="p">,(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)),</span><span class="n">gaba_mat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Get the pre-synaptic activation function for only the existing synapses</span>
    
    <span class="n">do_gabadt</span> <span class="o">=</span> <span class="n">alp_gaba</span><span class="o">*</span><span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">o_gaba</span><span class="p">)</span><span class="o">*</span><span class="n">T_gaba</span> <span class="o">-</span> <span class="n">bet_gaba</span><span class="o">*</span><span class="n">o_gaba</span> <span class="c1"># Calculate the derivative of the open fraction of the GABAa synapses</span>
    
    <span class="c1">## Updation for fire times ##</span>
    
    <span class="n">dfdt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">fire_t</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">fire_t</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="c1"># zero change in fire_t as it will be updated by the modified integrator</span>
    
    <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">dVdt</span><span class="p">,</span><span class="n">dmdt</span><span class="p">,</span><span class="n">dhdt</span><span class="p">,</span><span class="n">dndt</span><span class="p">,</span><span class="n">do_achdt</span><span class="p">,</span><span class="n">do_gabadt</span><span class="p">,</span><span class="n">dfdt</span><span class="p">],</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># Concatenate the derivatives of the membrane potential, gating variables, and open fractions</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="updaing-the-gating-variable-and-the-initial-conditions">
<h3>Updaing the Gating Variable and the Initial Conditions<a class="headerlink" href="#updaing-the-gating-variable-and-the-initial-conditions" title="Permalink to this headline">¶</a></h3>
<p>As before, we again define functions that return the values of <span class="math notranslate nohighlight">\(\tau_m\)</span>, <span class="math notranslate nohighlight">\(\tau_h\)</span>, <span class="math notranslate nohighlight">\(\tau_n\)</span>, <span class="math notranslate nohighlight">\(m_0\)</span>, <span class="math notranslate nohighlight">\(h_0\)</span>, <span class="math notranslate nohighlight">\(n_0\)</span>, set parameters and initial conditions.</p>
<p>Note: The last firing times are stored in the <span class="math notranslate nohighlight">\(n\)</span> elements of the state vector. If we initialize the last firing time as 0, then the second neuron <span class="math notranslate nohighlight">\(X_2\)</span> will get an EPSP immediately after the start of the simulation. To avoid this the last firing time should be initialized to a large negative number greater than the duration of the simulation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">K_prop</span><span class="p">(</span><span class="n">V</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function determines the K-channel gating dynamics.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    V: float</span>
<span class="sd">        The membrane potential.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">T</span> <span class="o">=</span> <span class="mi">22</span> <span class="c1"># Temperature</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="mf">3.0</span><span class="o">**</span><span class="p">((</span><span class="n">T</span><span class="o">-</span><span class="mf">36.0</span><span class="p">)</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># Temperature-correction factor</span>
    <span class="n">V_</span> <span class="o">=</span> <span class="n">V</span><span class="o">-</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">)</span> <span class="c1"># Voltage baseline shift</span>
    
    <span class="n">alpha_n</span> <span class="o">=</span> <span class="mf">0.02</span><span class="o">*</span><span class="p">(</span><span class="mf">15.0</span> <span class="o">-</span> <span class="n">V_</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="mf">15.0</span> <span class="o">-</span> <span class="n">V_</span><span class="p">)</span><span class="o">/</span><span class="mf">5.0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="c1"># Alpha for the K-channel gating variable n</span>
    <span class="n">beta_n</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="mf">10.0</span> <span class="o">-</span> <span class="n">V_</span><span class="p">)</span><span class="o">/</span><span class="mf">40.0</span><span class="p">)</span> <span class="c1"># Beta for the K-channel gating variable n</span>
    
    <span class="n">t_n</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">((</span><span class="n">alpha_n</span><span class="o">+</span><span class="n">beta_n</span><span class="p">)</span><span class="o">*</span><span class="n">phi</span><span class="p">)</span> <span class="c1"># Time constant for the K-channel gating variable n</span>
    <span class="n">n_0</span> <span class="o">=</span> <span class="n">alpha_n</span><span class="o">/</span><span class="p">(</span><span class="n">alpha_n</span><span class="o">+</span><span class="n">beta_n</span><span class="p">)</span> <span class="c1"># Steady-state value for the K-channel gating variable n</span>
    
    <span class="k">return</span> <span class="n">n_0</span><span class="p">,</span> <span class="n">t_n</span>


<span class="k">def</span> <span class="nf">Na_prop</span><span class="p">(</span><span class="n">V</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function determines the Na-channel gating dynamics.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    V: float</span>
<span class="sd">        The membrane potential.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">T</span> <span class="o">=</span> <span class="mi">22</span> <span class="c1"># Temperature </span>
    <span class="n">phi</span> <span class="o">=</span> <span class="mf">3.0</span><span class="o">**</span><span class="p">((</span><span class="n">T</span><span class="o">-</span><span class="mi">36</span><span class="p">)</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># Temperature-correction factor</span>
    <span class="n">V_</span> <span class="o">=</span> <span class="n">V</span><span class="o">-</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">)</span> <span class="c1"># Voltage baseline shift</span>
    
    <span class="n">alpha_m</span> <span class="o">=</span> <span class="mf">0.32</span><span class="o">*</span><span class="p">(</span><span class="mf">13.0</span> <span class="o">-</span> <span class="n">V_</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="mf">13.0</span> <span class="o">-</span> <span class="n">V_</span><span class="p">)</span><span class="o">/</span><span class="mf">4.0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="c1"># Alpha for the Na-channel gating variable m</span>
    <span class="n">beta_m</span> <span class="o">=</span> <span class="mf">0.28</span><span class="o">*</span><span class="p">(</span><span class="n">V_</span> <span class="o">-</span> <span class="mf">40.0</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">V_</span> <span class="o">-</span> <span class="mf">40.0</span><span class="p">)</span><span class="o">/</span><span class="mf">5.0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="c1"># Beta for the Na-channel gating variable m</span>
    
    <span class="n">alpha_h</span> <span class="o">=</span> <span class="mf">0.128</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="mf">17.0</span> <span class="o">-</span> <span class="n">V_</span><span class="p">)</span><span class="o">/</span><span class="mf">18.0</span><span class="p">)</span> <span class="c1"># Alpha for the Na-channel gating variable h</span>
    <span class="n">beta_h</span> <span class="o">=</span> <span class="mf">4.0</span><span class="o">/</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="mf">40.0</span> <span class="o">-</span> <span class="n">V_</span><span class="p">)</span><span class="o">/</span><span class="mf">5.0</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="c1"># Beta for the Na-channel gating variable h</span>
    
    <span class="n">t_m</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">((</span><span class="n">alpha_m</span><span class="o">+</span><span class="n">beta_m</span><span class="p">)</span><span class="o">*</span><span class="n">phi</span><span class="p">)</span> <span class="c1"># Time constant for the Na-channel gating variable m</span>
    <span class="n">t_h</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">((</span><span class="n">alpha_h</span><span class="o">+</span><span class="n">beta_h</span><span class="p">)</span><span class="o">*</span><span class="n">phi</span><span class="p">)</span> <span class="c1"># Time constant for the Na-channel gating variable h</span>
    
    <span class="n">m_0</span> <span class="o">=</span> <span class="n">alpha_m</span><span class="o">/</span><span class="p">(</span><span class="n">alpha_m</span><span class="o">+</span><span class="n">beta_m</span><span class="p">)</span> <span class="c1"># Steady-state value for the Na-channel gating variable m</span>
    <span class="n">h_0</span> <span class="o">=</span> <span class="n">alpha_h</span><span class="o">/</span><span class="p">(</span><span class="n">alpha_h</span><span class="o">+</span><span class="n">beta_h</span><span class="p">)</span> <span class="c1"># Steady-state value for the Na-channel gating variable h</span>
    
    <span class="k">return</span> <span class="n">m_0</span><span class="p">,</span> <span class="n">t_m</span><span class="p">,</span> <span class="n">h_0</span><span class="p">,</span> <span class="n">t_h</span>


<span class="c1"># Initializing the Parameters</span>

<span class="n">C_m</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>         <span class="c1"># Membrane capacitances</span>
<span class="n">g_K</span> <span class="o">=</span> <span class="p">[</span><span class="mf">10.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>        <span class="c1"># K-channel conductances</span>
<span class="n">E_K</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">95.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>       <span class="c1"># K-channel reversal potentials</span>

<span class="n">g_Na</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>        <span class="c1"># Na-channel conductances</span>
<span class="n">E_Na</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>         <span class="c1"># Na-channel reversal potentials</span>

<span class="n">g_L</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.15</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>        <span class="c1"># Leak conductances</span>
<span class="n">E_L</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">55.0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span>       <span class="c1"># Leak reversal potentials</span>


<span class="c1"># Initializing the State Vector</span>

<span class="n">y0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="o">-</span><span class="mi">71</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span><span class="o">+</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span><span class="o">+</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n_ach</span><span class="o">+</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n_gaba</span><span class="o">+</span><span class="p">[</span><span class="o">-</span><span class="mi">9999999</span><span class="p">]</span><span class="o">*</span><span class="n">n_n</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="c1"># Initial state vector</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="creating-the-current-input-and-run-the-simulation">
<h3>Creating the Current Input and Run the Simulation<a class="headerlink" href="#creating-the-current-input-and-run-the-simulation" title="Permalink to this headline">¶</a></h3>
<p>We will run an 700 ms simulation with 100ms current injection at neuron <span class="math notranslate nohighlight">\(X_1\)</span> of increasing amplitude with 100ms gaps in between.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c1"># The step size for the numerical integration</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">700</span><span class="p">,</span><span class="n">epsilon</span><span class="p">)</span> <span class="c1"># The time points for the numerical integration</span>

<span class="n">current_input</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_n</span><span class="p">,</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="c1"># The current input to the network</span>
<span class="n">current_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="mi">100</span><span class="o">/</span><span class="n">epsilon</span><span class="p">):</span><span class="nb">int</span><span class="p">(</span><span class="mi">200</span><span class="o">/</span><span class="n">epsilon</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">current_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="mi">300</span><span class="o">/</span><span class="n">epsilon</span><span class="p">):</span><span class="nb">int</span><span class="p">(</span><span class="mi">400</span><span class="o">/</span><span class="n">epsilon</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">current_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="mi">500</span><span class="o">/</span><span class="n">epsilon</span><span class="p">):</span><span class="nb">int</span><span class="p">(</span><span class="mi">600</span><span class="o">/</span><span class="n">epsilon</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">7.5</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">odeint</span><span class="p">(</span><span class="n">dXdt</span><span class="p">,</span><span class="n">y0</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="n">n_n</span><span class="p">,</span><span class="n">F_b</span><span class="p">)</span> <span class="c1"># Solve the Differential Equation</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c1"># # Since we are using variables we have to initialize them</span>
    <span class="c1"># tf.global_variables_initializer().run()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualizing-and-interpreting-the-output">
<h3>Visualizing and Interpreting the Output<a class="headerlink" href="#visualizing-and-interpreting-the-output" title="Permalink to this headline">¶</a></h3>
<p>We overlay the voltage traces for the three neurons and observe the dynamics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the membrane potentials</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">state</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;$X_1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">state</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;$X_2$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">state</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;$X_3$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Simple Network $X_1$ --&gt; $X_2$ --◇ $X_3$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">90</span><span class="p">,</span><span class="mi">60</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (in ms)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Voltage (in mV)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Day 4_21_0.png" src="../../_images/Day 4_21_0.png" />
</div>
</div>
<p>We can see that the current injection triggers the firing of action potentials with increasing frequency with increasing current. Also we see that as soon as the first neuron <span class="math notranslate nohighlight">\(X_1\)</span> crosses its fireing threshold, an EPSP is triggered in the next neuron <span class="math notranslate nohighlight">\(X_2\)</span> causing a firing with a slight delay from the firing of <span class="math notranslate nohighlight">\(X_1\)</span>. Finally, as the second neuron depolarizes, we see a corresponding hyperpolarization in the next neuron <span class="math notranslate nohighlight">\(X_3\)</span> caused by an IPSP. We can also plot the dynamics of the channels itself by plotting o_ach and o_gaba which are the 5th and 4th last elements respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the fraction of open channels</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">state</span><span class="p">[:,</span><span class="o">-</span><span class="mi">5</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;$[O]_</span><span class="si">{ach}</span><span class="s2">$ in $X_2$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">state</span><span class="p">[:,</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;$[O]_</span><span class="si">{gaba}</span><span class="s2">$ in $X_3$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Channel Dynamics in $X_1$ --&gt; $X_2$ --◇ $X_3$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (in ms)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of Channels open&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Day 4_23_0.png" src="../../_images/Day 4_23_0.png" />
</div>
</div>
<p>Thus we are now capable of making complex networks of neurons with both excitatory and inhibitory connections.</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<p>(<a id="cit-Destexhe1994" href="#call-Destexhe1994">Destexhe, Mainen <em>et al.</em>, 1994</a>) Destexhe Alain, Mainen Zachary F. and Sejnowski Terrence J., ``<em>Synthesis of models for excitable membranes, synaptic transmission and neuromodulation using a common kinetic formalism</em>’’, Journal of Computational Neuroscience, vol. 1, number 3, pp. 195–230,  1994.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Tutorial\Day 4 Neurons and Networks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../Day%203%20Cells%20in%20Silicon/Day%203.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Day 3: Cells in Silicon</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../Day%205%20Optimal%20Mind%20Control/Day%205.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Day 5: Optimal Mind Control</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Rishika Mohanta and Collins Assisi<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>