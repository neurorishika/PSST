{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSVSvuofR5bW"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neurorishika/PSST/blob/master/Tutorial/Optional%20Material/Distributed%20TensorFlow/Distributed%20TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neurorishika/PSST/master/Tutorial/Optional%20Material/Distributed%20TensorFlow/Distributed%20TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKRfsqv4LrJd"
   },
   "source": [
    "## Day 7: (Optional) Distributed Computing with TensorFlow\n",
    "\n",
    "TensorFlow supports distributed computing, allowing portions of the graph to be computed on different processes, which may be on completely different servers! In addition, this can be used to distribute computation to servers with powerful GPUs, and have other computations done on servers with more memory, and so on. Unfortunately, the official documentation on Distributed TensorFlow rather jumps in at the deep end. For a slightly more gentle introduction we will run through some really basic examples with Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZcKCROILrJe",
    "outputId": "a2a68992-a428-4c38-fd57-a4bb12502904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "## OR ##\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8yKnLIaLrJf"
   },
   "source": [
    "Most times when we write a distributed code, we want each server to have access to a common set of variables. Say we want to share the variable var between two sessions (called sess1 and sess2) created on two different processes on different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPqT9o4-LrJg"
   },
   "outputs": [],
   "source": [
    "var = tf.Variable(initial_value=0.0)\n",
    "\n",
    "# Imagine this was run on server 1\n",
    "sess1 = tf.Session()\n",
    "sess1.run(tf.global_variables_initializer())\n",
    "\n",
    "# Imagine this was run on server 1\n",
    "sess2 = tf.Session()\n",
    "sess2.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pkcWoPbLrJg"
   },
   "source": [
    "Whenever a call to tf.Session() is made, it creates a completely seperate \"execution engine\". It is then connected to the session handle and the execution engine that stores variable values and runs operations. Lets try making changes on the variable var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86bfQdRDLrJg",
    "outputId": "3b285fd3-90f2-4e93-c049-42666f85c2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of var (Session 1): 0.0\n",
      "Value of var (Session 2): 0.0\n",
      "Increment var in Session 1\n",
      "Value of var (Session 1): 1.0\n",
      "Value of var (Session 2): 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Value of var (Session 1):\", sess1.run(var))\n",
    "print(\"Value of var (Session 2):\", sess2.run(var))\n",
    "\n",
    "sess1.run(var.assign_add(1.0))\n",
    "print(\"Increment var in Session 1\")\n",
    "\n",
    "print(\"Value of var (Session 1):\", sess1.run(var))\n",
    "print(\"Value of var (Session 2):\", sess2.run(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy0uCAHwLrJh"
   },
   "source": [
    "Thus, we can see, sessions in different processes are unlinked. Changing var in one session (on one execution engine) won't affect var in the other session. In order to share variables between processes, we need to link the different execution engines together. This is where we introduce Distributed TensorFlow.\n",
    "\n",
    "### Distributed TensorFlow\n",
    "\n",
    "TensorFlow works a bit like a server-client model. The idea is that the users creates a whole bunch of worker nodes that will perform the heavy lifting. A session is then created on one of those worker nodes, and it will compute the graph, possibly distributing parts of it to other worker nodes on the cluster.\n",
    "\n",
    "In order to do this, the main worker, needs to know about the other workers. This is done via the creation of a \"ClusterSpec\", which you need to pass to all workers. A ClusterSpec is built using a dictionary, where the key is a “job name”, and each job contains many workers.\n",
    "\n",
    "The first step is to define what the cluster looks like. We start off with the simplest possible cluster: two worker nodes/servers, both on the same machine; one that will listen on port 2020, one on port 2021. And we create a job called \"local\" using these servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-oB8f1WLrJi"
   },
   "outputs": [],
   "source": [
    "# Define the Worker Nodes (called Tasks)\n",
    "tasks = [\"localhost:2020\", \"localhost:2021\"]\n",
    "# Define the Cluster Jobs which is a dictionary of connect tasks\n",
    "jobs = {\"local\":tasks}\n",
    "# Initialize the Cluster using ClusterSpec\n",
    "cluster = tf.train.ClusterSpec(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6nNSf7yLrJj"
   },
   "source": [
    "We now launch the servers associated with the cluster jobs using the Server function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3etGMvFuLrJk"
   },
   "outputs": [],
   "source": [
    "# This server corresponds to the the first worker associated with the 'local' job.\n",
    "s1 = tf.train.Server(cluster, job_name=\"local\", task_index=0)\n",
    "# This server corresponds to the the second worker associated with the 'local' job.\n",
    "s2 = tf.train.Server(cluster, job_name=\"local\", task_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I0gfRSJLrJk"
   },
   "source": [
    "With the servers linked together in the same cluster, variables in any one of the server will be shared between all servers. By default, variables and operations get stored and executed on the first worker in the cluster. but to fix a variable or an operation to a specific worker, we can use tf.device()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hVHzQp3LrJl"
   },
   "outputs": [],
   "source": [
    "# Place variable 'var' in the first server\n",
    "with tf.device(\"/job:local/task:0\"): \n",
    "  var = tf.Variable(initial_value=0.0, name='var')\n",
    "sess1 = tf.Session(s1.target)\n",
    "sess2 = tf.Session(s2.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pDvG4FeLrJl"
   },
   "source": [
    "We can now try the same thing we did earlier to change the value of var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QU_14QcPLrJl",
    "outputId": "1255d09a-b6bf-47de-8588-d85d31e08ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of var (Session 1): 0.0\n",
      "Value of var (Session 2): 0.0\n",
      "Increment var in Session 1\n",
      "Value of var (Session 1): 1.0\n",
      "Value of var (Session 2): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the variables\n",
    "sess1.run(tf.global_variables_initializer())\n",
    "sess2.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"Value of var (Session 1):\", sess1.run(var))\n",
    "print(\"Value of var (Session 2):\", sess2.run(var))\n",
    "\n",
    "sess1.run(var.assign_add(1.0))\n",
    "print(\"Increment var in Session 1\")\n",
    "\n",
    "print(\"Value of var (Session 1):\", sess1.run(var))\n",
    "print(\"Value of var (Session 2):\", sess2.run(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrwsGaBMLrJm"
   },
   "source": [
    "Voila! Now the value of var is changed for both sessions. An interesting thing to note would be that the second tf.global_variables_initializer() is redundant as there is only a single shared variable that gets initialized by the first call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7UOit7MLrJm"
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/job:local/task:0\"):\n",
    "    var1 = tf.Variable(0.0, name='var1')\n",
    "with tf.device(\"/job:local/task:1\"):\n",
    "    var2 = tf.Variable(0.0, name='var2')\n",
    "    \n",
    "# (This will initialize both variables)\n",
    "sess1.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LQTJJIXLrJm"
   },
   "source": [
    "Here, whenever we use var1 it will always be run on the first task/worker node (localhost:2020) and for var2 it will always be run on the second task/worker node (localhost:2021).\n",
    "\n",
    "### Example\n",
    "\n",
    "Lets try to take a simple Tensorflow Computation graph and split it across multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AfuWs29TQbWw"
   },
   "outputs": [],
   "source": [
    "#@markdown Restart runtime\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZStlbzLLrJn",
    "outputId": "3d2c4cdd-a2e6-43e3-ecbf-ab08a92d640a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "## OR ##\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "x = tf.constant(2)\n",
    "y1 = x + 300\n",
    "y2 = x - 66\n",
    "y = y1 + y2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(y)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRVvCKBzLrJn"
   },
   "source": [
    "Now we will use Process() from the multiprocessing package to create workers on different processes and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iwQUVVLFRz8j"
   },
   "outputs": [],
   "source": [
    "#@markdown Restart runtime\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F28GjzGFLrJn"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "# Make a function that creates workers on \"localhost:2020\" \n",
    "# and \"localhost:2021\" given the worker number and joins\n",
    "\n",
    "def create_server(worker_number):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    ## OR ##\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_v2_behavior()\n",
    "    cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2020\", \"localhost:2021\"]})\n",
    "    worker = tf.train.Server(cluster, job_name=\"local\", task_index=worker_number)\n",
    "    print(\"Starting server #{}\".format(worker_number))\n",
    "    worker.start()\n",
    "    worker.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uNOP3fFLrJo"
   },
   "source": [
    "We then create and start the two processes using the function create_server and giving the worker number as the argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HTHSB9aLrJo"
   },
   "outputs": [],
   "source": [
    "# Create Process\n",
    "p1 = Process(target=create_server,args=(0,))\n",
    "p2 = Process(target=create_server,args=(1,))\n",
    "\n",
    "# Start Process\n",
    "p1.start()\n",
    "p2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xmbqL4sLrJo"
   },
   "source": [
    "Finally we actually run the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10EBB7wlLrJp",
    "outputId": "78809cd1-8e00-4de1-9788-c0b9a886d13b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "## OR ##\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Initialize the Cluster we are using\n",
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2020\", \"localhost:2021\"]})\n",
    "\n",
    "x = tf.constant(2)\n",
    "\n",
    "# define the device to use. FORMAT: /job:JOB_NAME/task:TASK_NUMBER\n",
    "\n",
    "with tf.device(\"/job:local/task:0\"):\n",
    "    y2 = x - 66\n",
    "\n",
    "with tf.device(\"/job:local/task:1\"):\n",
    "    y1 = x + 300\n",
    "    y = y1 + y2\n",
    "\n",
    "# Run session on one of the workers\n",
    "with tf.Session(\"grpc://localhost:2020\") as sess:\n",
    "    result = sess.run(y)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EUyIFnxLrJp"
   },
   "source": [
    "Thus we are now capable of running distributed code over TensorFlow. In an actual distributed scenario, we will be running the code defined in create_server() on different nodes of a cluster and run the last cell in the main worker node to actually perform the computation.\n",
    "\n",
    "From this example it is really easy to now break the integrator into different sections and run them on different nodes to optimize performance by distributing some intensive computation to servers with powerful GPUs, and have other memory heavy computations done on servers with more memory, and so on. A device can be specified on a remote computer by modifying the device string. As an example “/job:local/task:0/gpu:0” will target the GPU on the local job.\n",
    "\n",
    "Sources: \n",
    "https://databricks.com/tensorflow/distributed-computing-with-tensorflow\n",
    "http://amid.fish/distributed-tensorflow-a-gentle-introduction"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Distributed TensorFlow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}